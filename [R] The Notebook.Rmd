---
title: "[R] The Notebook"
author: "*Veneta Baeva*"  
output: github_document
---
![](/Users/venetabaeva/git/repository4/Picture1.jpg)
---

## I. 

1.[data_set] 

>load

```{r}
library(tidyverse)
library(dslabs)
```


1.1.1[data_set] create data frame

```{r}
data_frame_city_temperature <- data.frame(city=c("Beijing", "Lagos", "Paris", "Rio de Janeiro", "San Juan", "Toronto"),
                     temp = c(35, 88, 42, 84, 81, 30),
                     region = c("APAC", "EMEA", "EMEA","LATAM", "APAC", "NA"),
                     population = c(20000,30000, 50000,60000,80000,100000),
                     stringsAsFactors = FALSE)
```

1.1.2[data_set] read

```{r}
file_alchohol_consumption<- read.csv(
                      file = ("/Users/venetabaeva/git/repository4/gapminder.csv"),
                      header = TRUE,
                      sep = ";",
                      dec = ".")

```

###### ^1^ click: [Alcohol Consumption](https://www.kaggle.com/sansuthi/alcohol-consumption)

1.1.3.[data_set] check available

```{r}
data()
```

>load

```{r}
library(downloader) 
```

1.1.4.[data_set] download 

```{r}
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/femaleMiceWeights.csv"
female_mice_weight_csv <- "femaleMiceWeights.csv" 
download(url, destfile=female_mice_weight_csv)
female_mice_weight<- read.csv(female_mice_weight_csv, header = TRUE,sep = ",")
```

1.1.5.[data_set] download 

```{r}
library(downloader)
url="https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/msleep_ggplot2.csv"
msleep_file <- basename(url)
download(url,msleep_file)
msleep<- read.csv("msleep_ggplot2.csv")
class(msleep)
```

1.2.[data_set] check class 

```{r}
class(murders)
class(file_alchohol_consumption)
class(data_frame_city_temperature)
```

2.[data_frame] 

>load

```{r}
library(tidyverse)
library(dslabs)
```

2.1.[data_frame] load

```{r}
data(murders)
data(heights)
```

2.2.[data_frame] view

```{r}
View(murders)
View(heights)
View(file_alchohol_consumption)
View(data_frame_city_temperature)
```

2.3.[data_frame] check structure 

```{r}
str(murders)
str(heights)
str(file_alchohol_consumption)
str(data_frame_city_temperature)
```

2.4.[data_frame] check first rows with headers

```{r}
head(murders)
head(heights)
head(file_alchohol_consumption)
head(data_frame_city_temperature)
```

2.5.[data_frame_column] check headers

```{r}
names(murders)
colnames(murders)
names(heights)
names(file_alchohol_consumption)
names(data_frame_city_temperature)
```

2.6.[data_frame_column] check class 

```{r}
class(murders$state)
class(murders$total)
class(heights$height)
class(file_alchohol_consumption$abbrv)
class(data_frame_city_temperature$temp)
class(3L)
```

3.1.[vector] change class

```{r}
test <- c(data_frame_city_temperature$temp)
y<- as.numeric(test) 
x<- as.character(y)
class(x)
```

3.2.[vector] sort in increasing order 

```{r}
sort(murders$total) 
```

3.3 [vector] return/keep index of data values in increasing order

```{r}
index <- order(murders$total)
index
```

3.4.{vector} associate abbreviation to data value

```{r}
abbreviate_country <- c("AF"="Afghanistan", "AL" = "Albania", "DZ" ="Algeria", "AD" = "Andorra", "AO" = "Angola", "AG"= "Antigua and Barbuda")
abbreviate_country
number_country<-c("1"="Afghanistan","2"="Albania","3"="Algeria","4" = "Andorra","5" = "Angola", "6" = "Antigua and Barbuda")
number_country
value_country<-c("Afghanistan"= 1,"Albania" =2,"Algeria" = 3 ,"Andorra"=4, "Angola" = 5 ,  "Antigua and Barbuda"=6)
value_country
```

3.5.[vector] extract associated data values

```{r}
abbreviation_country<- names(abbreviate_country)
abbreviation_country
```

3.6. [vector] access data value through index

```{r}
abbreviate_country[2]
abbreviate_country[c(1,3)]
file_alchohol_consumption$country[2]
```

3.7.[vector] access interval of data values through index 

```{r}
abbreviate_country[1:2]
```

3.8.[vector] access interval of data values through associated to these data values

```{r}
abbreviate_country["AF"]
abbreviate_country[c("AL","DZ")]
```

3.8.[vector] access interval of data values through associated to these data values

```{r}
min(value_country)
max(value_country)
```

>load

```{r}
library(dplyr) 
```

3.9.[vector] add mutate  

```{r}
murders <- mutate(murders,rate=total/population*100000)
```

4.3.9.[vector] histogram 

```{r}
murders_rate <-murders$rate
hist(murders_rate) 
```

3.10.[vector] rank rate mutate  

```{r}
murders <- mutate(murders,rank=rank(-rate)) # in descending order
```

3.11.[vector]  mutate rate rank filter

```{r}
murders <- mutate(murders, rate = total/population * 100000, rank = rank(-rate))  
murders%>%
  filter(rank <=5)
```

3.12. [vector] filter count number of rows

```{r}
nrow( filter(murders,region!="South"))
```

3.13. [vector] filter count number of rows

```{r}
nrow(filter(murders,region %in% c("Northeast","West")))
```

3.14. [vector] select filter rate rank

```{r}
select(filter(murders,region %in% c("Northeast","West") & rate < 1),state,rate,rank)
```

3.15. [vector] filter select rate rank

```{r}
filter(select(murders,state,region,rate),rate <= 0.71)
```

3.15.1. [vector] filter select unlist 

```{r}
primates_msleep <- filter(msleep, order =="Primates") %>% 
  select(sleep_total)%>% 
  unlist 
mean(primates_msleep)
```

3.15.2. [vector] filter mean 

```{r}
hf_diet_female_mice_weight <- female_mice_weight[female_mice_weight$Diet == "hf",]
mean(hf_diet_female_mice_weight$Bodyweight)
```

3.16. [vector] pipe select filter

```{r}
murders %>% 
  select(state,region,rate) %>% 
  filter(rate <= 0.71)
```

3.17. [vector] pipe mutate filter select

```{r}
murders %>%
  mutate(rate=total/murders$population*100000, rank=rank(-rate)) %>% 
  filter(region %in% c('Northeast','West') & rate <1) %>% 
  select(state,rate,rank)
```

4.3.17. [vector] box - plot 

```{r}
boxplot(rate~region, data = murders)
boxplot(population~region, data = murders)
```

3.18. [vector]  filter select

```{r}
murders %>%
  select(population, total)

```

4.3.18.[vector] scatter plot : factor1 * factor2

```{r}
data(murders)
population_in_millions <- murders$population/10^6
total_gun_murders <- murders$total
plot(population_in_millions, total_gun_murders)
```

4.3.18.[vector] histogram : factor1 * factor2

```{r}
hist(population_in_millions)
```


4.3.18.[vector] scatter plot : factor1 * factor2 log10

```{r}
data(murders)
log10_population <- log10(murders$population)
log10_total_gun_murders <- log10(murders$total)
plot(log10_population,log10_total_gun_murders)
```

2.7.[data_frame_column] access data values

```{r}
murders$state #preserved column order in row
murders$population #preserved column order in row
murders["state"] #preserved column original
murders["population"] #preserved column original
```

2.8.[data_frame_column] count rows 

```{r}
length(murders$population)
lenght(heights$height)
lenght(file_alchohol_consumption$country)
lenght(data_frame_city_temperature$city)
```

2.9.[data_frame_column] show levels of factor

```{r}
levels(murders$region)
levels(heights$sex)
levels(file_alchohol_consumption$region)
levels(data_frame_city_temperature$region)
```

2.10.[data_frame_column] count levels of factor 

```{r}
length(levels(murders$region))
length(levels(heights$sex))
length(levels(file_alchohol_consumption$region))
length(levels(data_frame_city_temperature$region))
nlevels(murders$region)
nlevels(heights$sex)
nlevels(file_alchohol_consumption$region)
nlevels(data_frame_city_temperature$region)
```

2.11.[data_frame_column] simple frequency * level of factor 

```{r}
table(murders$region)
table(heights$sex)
table(file_alchohol_consumption$region)
table(data_frame_city_temperature$region)
```

2.12.[data_frame_column] simple frequency * level of 2 factors 

```{r}
table(murders$state,murders$region)
table(file_alchohol_consumption$country,file_alchohol_consumption$region)
table(data_frame_city_temperature$city,data_frame_city_temperature$region)
```

2.13.[vector] proportion

```{r}
proportion_murders <- murders$total/murders$population*100000 
print(proportion_murders)
```

2.13.[vector] validate data value meets expected value

```{r}
index_values <- proportion_murders<0.71
index_values
murders$state[index_values]
sum(index_values)
```

2.14.[vector] validate data value meets expected value

```{r}
murders_region_west<- murders$region == "West" 
murders_rate_safe <- proportion_murders <= 1 
index__rate_per_region <- murders_rate_safe&murders_region_west
murders$state[index__rate_per_region]
```

2.15.[vector] validate true/false data value

```{r}
which(index_values)
which(murders$state == "Massachusetts")
```

2.16.[vector] search match true/false data value

```{r}
match(c("New York","Florida","Texas"),murders$state)
murders$state[match(c("New York","Florida","Texas"),murders$state)]
proportion_murders[match(c("New York","Florida","Texas"),murders$state)]
```

2.17.[vector] check if each data value in vector 1 is in vector 2

```{r}
x<- c("a","b","c")
y <- c("a","b","f") 
y %in% x
```

2.18.[vector] check if  data value in vector 1 is/ is not  in vector 2

```{r}
c("Boston","Dakota","Washington") %in% murders$state
states<- c("Boston","Dakota","Washington")
index_not_present<- which(!states%in%murders$state)
states[index_not_present]
```

4.2.18.[data_frame] scatter plot

```{r}
murders %>%
  ggplot(aes(population, total, label=abb, color=region)) +
  geom_label()
file_alchohol_consumption %>%
  ggplot(aes(incomeper1, aconsum, label=abbrv, color=region)) +
  geom_label()
data_frame_city_temperature %>%
  ggplot(aes(population, temp, label=city, color=region)) +
  geom_label()
```

2.19.[vector] average vs. mean

```{r}
murders_total<- murders$total
average <- function(murders_total){
  sum_murders_total <- sum(murders_total)
  count_murders_total <- length(murders_total)
  sum_murders_total/count_murders_total
}
average(murders_total)
mean(murders_total)
```












1.[dataFrame] create data frame

```{r}
dfAbbrvAconsum <- data.frame(country = abbrv, alchoholconsumption = aconsum)
```

1.[file] check class

```{r}
class(dfAlc)
class(dfAlc$aconsum)
```

1.[element] change type of element 

```{r}
class(1)
class(1L)
```

1.[sequence] show

```{r}
seq(1,10)
1:10
```

1.[sequence] generate increasing sequence of numbers with pretermed length

```{r}
seq(1, 10, length.out = 100)
```


1.[sequence] check length

```{r}
length(seq(1,10))
```


1.[dataFrame] check levels of a factor

```{r}
levels(dfAlc$aconsum)
```

1.[dataFrame] check number of levels of a factor

```{r}
nlevels(dfAlc$aconsum)
```

1.[file] check headers

```{r}
head(dfAlc)
```

1.[file] check headers' names

```{r}
names(dfAlc)
```

1.[file] exact name 

```{r}
colnames(dfPonzo)
```

1.[file] rename exact name column

```{r}
colnames(dfPonzo)[colnames(dfPonzo) == 'X.Gaze.x...'] <- 'xGaze'
colnames(dfPonzo)[colnames(dfPonzo) == 'X.Gaze.y...'] <- 'yGaze'
colnames(dfPonzo)[colnames(dfPonzo) == 'X..RespondentNr.'] <- 'respondentNr'
colnames(dfPonzo)[colnames(dfPonzo) == 'X.StimulusNr.'] <- 'stimulusNr'
colnames(dfPonzo)[colnames(dfPonzo) == 'X.timestamp.ms.'] <- 'timeStampsMs'
```

1.[row-column] access value

```{r}
dfPonzo[12,3]
```

1.[column-row] access value

```{r}
dfAlc$country[11]
```


1.[column] access

```{r}
dfAlc["aconsum"]
```

1.[vector]sort in increasing order 

```{r}
sort(dfAlc$aconsum)
```


1.[column] access through vector

```{r}
dfAlc[["aconsum"]]
```


1.[column] access through vector

```{r}
dfAlc$aconsum
```

1.[object] define

```{r}
aconsum <- dfAlc$aconsum
urbanrt <-dfAlc$urbanrt
employrt <- dfAlc$employrt
```

1.[vector] count NAs

```{r}
naS <- is.na(aconsum)
sum(naS)
```

1.[vector] expell NAs

```{r}
mean(aconsum[!naS])
```

1.[vector] point column from data frame to calculate the mean for

```{r}
mean(dfAlc[,2])
```


1.[dataFrame] filter, summarize

```{r}
dfAlc %>%
  filter(region == "EMEA") %>%
  summarize(median = median(aconsum), #call only  functions that return a single value
            minimum = min(aconsum),
            maximum = max(aconsum)) 
```

1.[dataFrame] filter, summarize, access  summarized numeric value

```{r}
dfAlcEMEAMedian<- dfAlc %>%
  filter(region == "EMEA") %>%
  summarize(median = median(aconsum), 
            minimum = min(aconsum),
            maximum = max(aconsum))%>%
  .$median #the dot as a placeholder for the data that is being passed through the pipe
```

###### ^1^ Note: most of the dplyr functions, including summarize, always return data frames; if to be used the result with a function that requires a numeric value,  won't be able to do it
###### ^1^ Note:  .$median is equivalent to dfAlcEMEAMedian$median

1.[object] define length

```{r}
length(aconsum)
```

1.[vector] return number of elements

```{r}
lengthAconsum <- length(dfAlc$aconsum)
lengthAconsum
```

1.[object] check identical

```{r}
identical(urbanrt,employrt)
```

1.[vector] return index of the elements' values in increasing order 

1.1.[vector] sort vector's elements in order, through indexing

```{r}
iOrdDfAconsum<-order(dfAlc$aconsum)
dfAlc$abbrv[iOrdDfAconsum]
```

1.[vector] find max value between elements, through indexing

1.1[object] return index

1.1.1[vector] show value of element, through indexing the object 

```{r}
iMax<-which.max(dfAlc$aconsum)
iMax
dfAlc$abbrv[iMax]
dfAlc$abbrv[which.max(dfAlc$aconsum)]
```

1.[vector] find max value between elements, through indexing

1.1[object] return index

1.1.1[vector] show value of element, through indexing the object 

```{r}
iMin<-which.max(dfAlc$aconsum)
iMin
dfAlc$abbrv[iMin]
dfAlc$abbrv[which.min(dfAlc$aconsum)]
```

1.[vector] order

```{r}
dfAlc$abbrv[order(dfAlc$employrt,decreasing=TRUE)]
```

1.[vector] check range through indexing

1.1.[vector] count the number of elements in the range

```{r}
i <- dfAlc$aconsum  < 5
i
dfAlc$abbrv[i]
sum(i,na.rm =TRUE)
```

1.1.[vector] filter data by sub-setting row

```{r}
dplyr::filter(dfAlc, region=="EMEA")
```

1.[vector] filter

```{r}
emea<- dfAlc$region == "EMEA" 
aconsum <- dfAlc$aconsum <= 5 
i <- aconsum&emea
dfAlc$abbrv[i]
```

1.1.[vector] which indexes of elements  has value TRUE 

```{r}
which(i)
```

1.1.[vector] which value TRUE or FALSE of element 

```{r}
i<-which(dfAlc$abbrv == "BG")
i
aconsum[i]
```

1.1.[vector] match values of elements and return TRUE or FALSE 

```{r}
i <- match(c("BG","IT","ES"),dfAlc$abbrv)
i
dfAlc$abbrv[i]
aconsum[i]
```

1.1.[vector] check whether values of vector in values of vector and return TRUE or FALSE 

```{r}
emea<- dfAlc$region == "EMEA" 
aconsum5 <- dfAlc$aconsum <= 5 
aconsum10 <- dfAlc$aconsum <= 5 
aconsum5&emea %in% aconsum10&emea
```

1.1.[vector] check whether each value of vector in values of the same vector and return TRUE or FALSE 

```{r}
checkAbbrv<- c("GB","BG","MZ") %in% dfAlc$abbrv
```

1.1.[vector] check whether each value of vector in values of the same vector, through indexing and return TRUE or FALSE 

```{r}
checkAbbrv<- c("GB","BG","MZ") %in% dfAlc$abbrv
i <- which(!checkAbbrv%in%dfAlc$abbrv)
i
checkAbbrv[i] 
```

1.1.[dataFrame] mutate data frame

```{r}
dfAlc <- dplyr::mutate(dfAlc,rank=rank(-dfAlc$aconsum))
str(dfAlc)
```
###### ^2^ Note: include a column named rank with the ranks of rate from highest to lowest

1.1.[dataFrame] excluding filter value and create data frame 

```{r}
dfNoEMEA <- data.frame(dplyr::filter(dfAlc,region!="EMEA"))
nrow(dfNoEMEA)
```

1.1.[dataFrame] including filter value and create data frame 

```{r}
dfEMEAAPAC <- data.frame(dplyr::filter(dfAlc,region %in% c("EMEA","APAC")))
nrow(dfEMEAAPAC)
```

1.1.[dataFrame] filter and select only

```{r}
EMEAAPACAconsum10 <- dplyr::filter(dfAlc,region %in% c("EMEA","APAC") & aconsum < 10)
dplyr::select(EMEAAPACAconsum10,country,aconsum,rank)
```

1.1.[dataFrame] select subsetting and filter 

```{r}
newTable <- dplyr::select(dfAlc,country,region,aconsum) 
dplyr::filter(newTable,aconsum <= 10)
str(newTable)
```
1.[dataFrame] pipe select and pipe filter

```{r}
dfAlc %>% dplyr::select(country,region,aconsum)  %>% filter(aconsum <= 10)
```

1.[dataFrame] filter pipe select

```{r}
dplyr::filter(dfAlc, region %in% c("EMEA", "APAC") & aconsum < 10 )%>% dplyr::select(country, aconsum, rank)
```

1.[dataFrame] pipe filter select

```{r}
dfAlc %>% mutate(aconsum, rank) %>% dplyr::filter(region %in% c('EMEA','APAC') & aconsum <10) %>% dplyr::select(country,aconsum,rank)
```

1.[dataFrame]

```{r}
ind <- heights$height > mean(heights$height)#How many individuals in the dataset are above average height?
sum(ind)
ind <- heights$height > mean(heights$height) & (heights$sex =="Female")#How many individuals in the dataset are above average height and are female?
sum(ind)
mean(heights$sex == "Female")# proportion of individuals in the dataset are female
minH<- min(heights$height) 
ind <- match(minH, heights$height)#Use the match() function to determine the index of the first individual with the minimum height.
heights$sex[ind]#Subset the sex column of the dataset by the index in 4b to determine the individualâ€™s sex.
maxH <- max(heights$height)#Write code to create a vector x that includes the integers between the minimum and maximum heights (as numbers).
minH <- min(heights$height)
intgr <- c(minH:maxH)
intgr
sum(!(intgr %in% heights$height))#How many of the integers in x are NOT heights in the dataset?
heights <- mutate(heights, ht_cm = height*2.54)#create a new column of heights in centimeters named ht_cm
head(heights)
heights$ht_cm[18]
mean(heights$ht_cm) 
females <- dplyr::filter(heights, sex == "Female") #females by filtering the heights2 data to contain only female individuals.
head(females)
nrow(females)#How many females are in the heights2 dataset?
mean(females$ht_cm)#What is the mean height of the females in centimeters?
```

3.[if-els] 

```{r}
i <- which.min(dfAlc$aconsum)
if (dfAlc$aconsum[i] < 10){
  print(dfAlc$country[i])
}else{
  print("No coutnry has alcochol rate that low")
}
```

3.[ifelse] if logical is TRUE, then return 1st answer, if FALSE, then 2nd answer

```{r}
minAlc <- which.min(dfAlc$aconsum)
ifelse(minAlc,dfAlc$country,NA)
```

2.[dataFrame]remove NA

```{r}
sum(is.na(dfAlc))
dfAlc[is.na(dfAlc)] = 0
View(dfAlc)
```

3.[ifelse][all]

```{r}
if(all(dfAlc$aconsum < 30)){
  print("World Alcochol rate is under 30")
} else{
  print("World Alcochol rate is not under 30")
}
```

3.[ifelse]

```{r}
str(dfAlc)
x <- dfAlc$country
xCharCountry <- nchar(x)
y <- dfAlc$abbrv
changeCountryToAbbrv<- ifelse(xCharCountry >2,y,x)
changeCountryToAbbrv
```

3.[any] take a vector of logicals ; return true, if any of the entries is true 

```{r}
x <- c(TRUE, FALSE)
any(x)
```

3.[all] take a vector of logicals; return  TRUE, if all the entries are true 

```{r}
x <- c(TRUE, FALSE)
all(x)
```

4.[function]

```{r}
avg <- function(x){
  s <- sum(x)
  n <- length(x)# Note:  not saved in the work space; the values of the variables are changed only during the F's call
  s/n
} 
x<- dfAlc$urbanrt
identical (mean(x),avg(x))
```

4.[function]

```{r}
avg <- function(x,arithmetic=TRUE){ # calculate either geometric, or arithmetic average depending on predefined variable
    n <- length(x)
    arithmetic <-sum(x)/n
    geometric<- prod(x)^(1/n)
    ifelse(arithmetic,arithmetic,geometric)
  }
x<- dfAlc$urbanrt
avg(x)
```

3.[forloop]

```{r}
compute_s_n <- function(n){
    x<- 1:n
    sum(x) 
  }
compute_s_n(3)
m <-25 
s_n <- vector(length = m)#create an empty vector for storing
for(n in 1:m){
  s_n[n] <- compute_s_n(n)
}
n <- 1:m
plot(n,s_n)
lines(n,n*(n+1)/2)
```

3.[forloop]

```{r}
sum <- 0
for(i in 1:2) 
  sum <- sum + i^2
sum
```


2.[ggplot]

```{r}
abbCountry <- dfAlc$abbrv 
suicidesPer100 <- dfAlc$suicideper100
urbanRT <- dfAlc$urbanrt
region <- dfAlc$region
ranksAConsumption <- rank(dfAlc$aconsum,na.last = NA)
i <- order(dfAlc$aconsum)
df<- data.frame(country = abbCountry[i], suicide = suicidesPer100[i], rank = ranksAConsumption[i],urbanrate = urbanRT[i],region = region[i])
df %>%
  ggplot(aes(urbanrate, suicide, label=country,color=rank)) + geom_label()

```

2.[ggplot]

```{r echo=TRUE, message=FALSE, warning=FALSE}
dfAlc %>%
  ggplot(aes(urbanrt, employrt, label=abbrv, color=region)) + geom_label()
```

2.[ggplot]

```{r echo=TRUE, message=FALSE, warning=FALSE}
dfAlc %>%
  ggplot(aes(urbanrt, employrt, label=abbrv, color=region)) + geom_label()
```

2.[scatterPlot]

```{r}
plot(dfAlc$suicideper100,dfAlc$aconsum, xlab = "suicides/100 people", ylab="alcohol consumption")
plot(dfAlc$employrt,dfAlc$aconsum, xlab = "employee rate", ylab="alcohol consumption")
plot(dfAlc$urbanrt,dfAlc$aconsum, xlab = "urban rate", ylab="alcohol consumption")
```

2.[scatterPlot] in logs

```{r}
log10IncomePer1 <- log10(dfAlc$incomeper1)
log10Aconsum <- log10(dfAlc$aconsum)
plot(log10IncomePer1,log10Aconsum)
```

2.[histogram]

```{r}
hist(dfAlc$aconsum,xlab = "alcohol consumption") 
dfAlc$country[which.max(dfAlc$aconsum)]
```

2.[boxplot]

```{r}
boxplot(aconsum~region, data = dfAlc,na.action = NULL) 
boxplot(suicidesPer100~region, data = dfAlc)
boxplot(employrt~region, data = dfAlc)
boxplot(urbanrt~region, data = dfAlc)
```


## II.

## Libraries

> library load 

```{r}
library(UsingR)
```

1.[dataFrame]filter

```{r}
controlsXGaze <- dplyr::filter(dfPonzo, stimulusNr == c(1,3,13,15,25,27)) %>% dplyr::select(xGaze) %>% summarise(mean(xGaze))
controlsXGaze
controlsYGaze <-dplyr::filter(dfPonzo, stimulusNr == c(1,3,13,15,25,27)) %>% dplyr::select(yGaze) %>% summarise(mean(yGaze))
controlsYGaze
```

1.[dataFrame] pipe filter select unlist

```{r}
controlsXGaze <- dplyr::filter(dfPonzo, stimulusNr == c(1,3,13,15,25,27)) %>% dplyr::select(xGaze) 
mean(controlsXGaze$xGaze)
controlsYGaze <-dplyr::filter(dfPonzo, stimulusNr == c(1,3,13,15,25,27)) %>% dplyr::select(yGaze)
mean(controlsYGaze$yGaze)
longUpXGaze  <-dplyr::filter(dfPonzo, stimulusNr == c(5,7,17,19,29,31))%>% dplyr::select(xGaze)
mean(longUpXGaze$xGaze)
longUpYGaze  <-dplyr::filter(dfPonzo, stimulusNr == c(5,7,17,19,29,31))%>% dplyr::select(yGaze)
mean(longUpYGaze$yGaze)
longDownXGaze<- dplyr::filter(dfPonzo, stimulusNr == c(9,11,21,23,33,35))%>% dplyr::select(xGaze)
mean(longDownXGaze$xGaze)
longDownYGaze<- dplyr::filter(dfPonzo, stimulusNr == c(9,11,21,23,33,35))%>% dplyr::select(yGaze)
mean(longDownYGaze$yGaze)
```

1.[dataFrame] filter select

```{r}
controls <- dplyr::filter(dfPonzo, stimulusNr == c(1,3,13,15,25,27)) 
head(controls)
View(controls)
mean(controls$xGaze)
mean(controls$yGaze)
longUp  <-dplyr::filter(dfPonzo, stimulusNr == c(5,7,17,19,29,31))
head(longUp)
View(longUp)
mean(longUp$xGaze)
mean(longUp$yGaze)
longDown <- dplyr::filter(dfPonzo, stimulusNr == c(9,11,21,23,33,35))
head(longDown)
View(longDown)
mean(longDown$xGaze)
mean(longDown$yGaze)
```

1.[dataFrame] turn into numeric vector

```{r}
unlist(controls)
unlist(longUp)
unlist(longDown)
```

3.[plot]

```{r}
plot(controls$xGaze,controls$yGaze,xlab = "controlsXGaze", ylab="controlsYGaze") 
plot(longUp$xGaze,longUp$yGaze,xlab = "longUpXGaze", ylab="longUpYGaze") 
plot(longDown$xGaze,longDown$yGaze,xlab = "longDownXGaze", ylab="longDownYGaze") 
```

1.[population]get different random sample of 12;random variable of random sample

```{r}
popDfPonzoXGaze <- unlist(dfPonzo$xGaze)
popDfPonzoYGaze <- unlist(dfPonzo$yGaze)
mean(sample(popDfPonzoXGaze,12))
mean(sample(popDfPonzoYGaze,12))
mean(popDfPonzoXGaze)
mean(popDfPonzoYGaze)
```

1.[seed] set/ produce the same sample again and again = generate same set at each time

```{r}
set.seed(1) 
```

1.[average] difference between the average of the sample and the average of the pop

```{r}
sampleDfPonzoXGaze<- sample(popDfPonzoXGaze,5)
abs(mean(sampleDfPonzoXGaze)-mean(popDfPonzoXGaze))
sampleDfPonzoYGaze<- sample(popDfPonzoYGaze,5)
abs(mean(sampleDfPonzoYGaze)-mean(popDfPonzoYGaze))
```

1.[seed] set/ produce the same sample again and again = generate same set at each time by setting the starting number used to generate a sequence of random numbers

```{r}
set.seed(5) 
```

1.[average] difference between the average of the sample and the average of the pop

```{r}
sampleDfPonzoXGaze<- sample(popDfPonzoXGaze,5)
abs(mean(sampleDfPonzoXGaze)-mean(popDfPonzoXGaze))
sampleDfPonzoYGaze<- sample(popDfPonzoYGaze,5)
abs(mean(sampleDfPonzoYGaze)-mean(popDfPonzoYGaze))
```

###### ^1^ Note: the average of the samples is a random variable =>inferential statistics needed

1.[nullDistribution] all possible realizations under the null 

```{r}
obslongUpControlsX <- mean(longUp$xGaze) - mean(controls$xGaze)
obslongUpControlsY <- mean(longUp$yGaze) - mean(controls$yGaze)
obslongDownControls <- mean(longDown$xGaze) - mean(controls$xGaze)
obslongDownControls <- mean(longDown$yGaze) - mean(controls$yGaze)
popDfPonzoXGaze <- unlist(dfPonzo$xGaze)
popDfPonzoXGaze <- unlist(dfPonzo$yGaze)
popDfPonzoControls <- sample(popDfPonzoXGaze,12)
popDfPonzoLongUp <- sample(popDfPonzoXGaze,12)
mean(popDfPonzoLongUp) - mean(popDfPonzoControls)
popDfPonzoControlsY <- sample(popDfPonzoYGaze,12)
popDfPonzoLongUpY <- sample(popDfPonzoYGaze,12)
mean(popDfPonzoLongUpY) - mean(popDfPonzoControlsY)
```

###### ^1^ Note: do sampling and substraction multiple times = see several realizations of the difference in mean  for the null hypothesis

1.[nullHypothesis] check 

```{r}
n<-10000
nullsPopX <- vector("numeric",n)
for(i in 1:n){
  popDfPonzoControlsX <- sample(popDfPonzoXGaze,12)
  popDfPonzoLongUpX <- sample(popDfPonzoXGaze,12)
  nullsPopX[i]<- mean(popDfPonzoLongUpX) - mean(popDfPonzoControlsX)
}
max(nullsPopX)
hist(nullsPopX)
```

```{r}
n<-10000
nullsPopY <- vector("numeric",n)
for(i in 1:n){
  popDfPonzoControlsY <- sample(popDfPonzoYGaze,12)
  popDfPonzoLongUpY <- sample(popDfPonzoYGaze,12)
  nullsPopY[i]<- mean(popDfPonzoLongUpY) - mean(popDfPonzoControlsY)
}
max(nullsPopY)
hist(nullsPopY)
```

###### ^1^ Note: if knowing the null distribution, one can describe the proportion of values one sees for any interval of values -> define a number of times to redo the null hypothesis check; record all differences  

1.[nullHypothesis] #1 option: how often (frequency) null values are bigger or not than observed values 
```{r}
sum(nullsPopX > obslongUpControlsX)/n 
sum(nullsPopY > obslongUpControlsY)/n 
```

1.[nullHypothesis] #2 option: proportion of times the null is bigger than the observation 

```{r}
mean(nullsPopX > obslongUpControlsX)
mean(nullsPopY > obslongUpControlsY)
```

1.[pValue] probability that an outcome from the null distribution is bigger than what one observed when the null hypothesis is true 

```{r}
mean(abs(nullsPopX) >obslongUpControlsX)
mean(abs(nullsPopY) >obslongUpControlsY)
```

###### ^1^ Note: #3 option: how often (frequency) it is bigger in absolute 

1.[nullDistribution] loop for 1000 times taking a random sample of 5 gazes calculating the mean and storing it within an averages vector 

```{r}
head(popDfPonzoXGaze)
set.seed(1)
n<-1000
averagesPopX <- vector("numeric",n)
for(i in 1:n){
  X <- sample(popDfPonzoXGaze,5)#using a for-loop take a random sample of 5 gazes 1,000 times ; save these averages
  averagesPopX[i]<- mean(X)
}
```

```{r}
hist(averagesPopX)
```

1.[nullHypothesis] check 

```{r}
mean( abs( averagesPopX - mean(popDfPonzoXGaze) ) > 0.05) #What proportion of these 1,000 averages are more than 0.05%  away from the average of mean(X) ?
```

1.[nullDistribution] loop for 10000 times taking a random sample of 5 gazes calculating the mean and storing it within an averages vector 

```{r}
head(popDfPonzoXGaze)
set.seed(1)
n<-10000
averagesPopX <- vector("numeric",n)
for(i in 1:n){
  X <- sample(popDfPonzoXGaze,5)#using a for-loop take a random sample of 5 gazes 10000 times ; save these averages
  averagesPopX[i]<- mean(X)
}
```

```{r}
hist(averagesPopX)
```

1.[nullHypothesis] check 

```{r}
mean( abs( averagesPopX - mean(popDfPonzoXGaze) ) > 0.05) #What proportion of these 10000 averages are more than 0.05%  away from the average of mean(X) ?
```

1.[nullDistribution] loop for 10000 times taking a random sample of 5 gazes calculating the mean and storing it within an averages vector 

```{r}
head(popDfPonzoYGaze)
set.seed(1)
n<-1000
averagesPopY <- vector("numeric",n)
for(i in 1:n){
  Y <- sample(popDfPonzoYGaze,5)#using a for-loop take a random sample of 5 gazes 1,000 times ; save these averages
  averagesPopY[i]<- mean(Y)
}
```

```{r}
hist(averagesPopY)
```

1.[nullHypothesis] check 

```{r}
mean( abs( averagesPopY - mean(popDfPonzoYGaze) ) > 0.05)#What proportion of these 1,000 averages are more than 0.05%  away from the average of mean(Y) ?
```

1.[normalApproximation] describe distribution = describe the entier list

```{r}
head(popDfPonzoXGaze)
mean(popDfPonzoXGaze <= 0.500)
prop = function(q) {
  mean(popDfPonzoXGaze <= q)
}
qs<- seq(from = min(popDfPonzoXGaze), to = max(popDfPonzoXGaze), length=20)
props = sapply(qs,prop)
props = sapply(qs, function(q) mean(popDfPonzoXGaze <= q))
```

1.[normalApproximation] plot

```{r}
plot(qs, props)
```

###### ^1^ Note: #1 option: empirical cumulative distribution function 

1.[normalApproximation] describe distribution = describe the entier list => empirical cumulative distribution function 

```{r}
plot(ecdf(popDfPonzoXGaze))
```

###### ^1^ Note: #2 option: empirical cumulative distribution function 


1.[centralLimitTheorem] 

```{r}
head(popDfPonzoXGaze)
set.seed(1)
n<-1000
averagesPopX5 <- vector("numeric",n)
for(i in 1:n){
  X <- sample(popDfPonzoXGaze,5)#using a for-loop take a random sample of 5 mice 1,000 times ; Save these averages
  averagesPopX5[i]<- mean(X)
}
```

```{r}
head(popDfPonzoXGaze)
set.seed(1)
n<-1000
averagesPopX50 <- vector("numeric",n)
for(i in 1:n){
  X <- sample(popDfPonzoXGaze,50)#using a for-loop take a random sample of 5 mice 1,000 times ; Save these averages
  averagesPopX50[i]<- mean(X)
}
```

```{r}
par(mfrow = c(2,1))
hist(averagesPopX5)
hist(averagesPopX50)  
```

```{r}
mean(averagesPopX5<0.600 & averagesPopX5>0.400)
mean(averagesPopX50<0.600 & averagesPopX50>0.400)
```

###### ^1^ Note: for the last set of averages, the ones obtained from a sample size of 5 vs. 50, what proportion are between 0.400 and 0.600? 

```{r}
mu <- mean(averagesPopX50)
mu
sigma <- sd(averagesPopX50)
sigma
```

1. [pNorm] find  proportion of observations below a cutoff x given a normal distribution with mean mu and standard deviation sigma with pnorm(x, mu, sigma) or pnorm( (x-mu)/sigma )

```{r}
pnorm(0.600, mu, sigma) - pnorm(0.400,mu,sigma)  
```

###### ^1^ Note: What is the proportion of observations between 0.600 and 0.400 in a normal distribution with mu and sd?

1.[centralLimitTheorem]

```{r}
popMean <- mean(popDfPonzoXGaze)
popSD <- sd(popDfPonzoXGaze)
propWithinOneSDX<- (popDfPonzoXGaze-popMean)/popSD
mean(abs(propWithinOneSDX) <=1) #What proportion of xGaze% are within 1 SD away from the average Gaze%?
mean(abs(propWithinOneSDX) <=2)#What proportion of xGaze% are within 2 SD away from the average Gaze%?
mean(abs(propWithinOneSDX) <=3) #What proportion of xGaze% are within 3 SD away from the average Gaze%?
popMean <- mean(popDfPonzoYGaze)
popSD <- sd(popDfPonzoYGaze)
propWithinOneSDY<- (popDfPonzoYGaze-popMean)/popSD
mean(abs(propWithinOneSDY) <=1) #What proportion of xGaze% are within 1 SD away from the average Gaze%?
mean(abs(propWithinOneSDY) <=2)#What proportion of xGaze% are within 2 SD away from the average Gaze%?
mean(abs(propWithinOneSDY) <=3) #What proportion of xGaze% are within 3 SD away from the average Gaze%?
```

> library load 

```{r}
library(rafalib)
```

```{r}
qqnorm(popDfPonzoXGaze)
abline(0,1)
qqnorm(popDfPonzoYGaze)
abline(0,1)
```

> library load 

```{r}
library(dplyr)
```

```{r}
avgs <- replicate(10000, mean( sample(popDfPonzoXGaze, 25)))
mypar(1,2)
hist(avgs)
qqnorm(avgs)
qqline(avgs)
mean(avgs) #What is the average of the distribution of the sample average?
```

> library load 

```{r}
library(rafalib)
```

```{r}
sd(avgs)#What is the standard deviation of the distribution of sample averages (use popsd())?
```

> library load 

```{r}
library(dplyr)
```

1.[centralLimitTheorem] practice

###### ^1^ Note: the NUll distirbution is very well approximated by a normal distribution; checkwhether normal approximation applies

```{r}
obslongUpControlsX <- mean(longUp$xGaze) - mean(controls$xGaze)
obslongUpControlsY <- mean(longUp$yGaze) - mean(controls$yGaze)
popDfPonzoXGaze <- unlist(dfPonzo$xGaze)
popDfPonzoYGaze <- unlist(dfPonzo$yGaze)
```

```{r}
popDfPonzoControlsX <- sample(popDfPonzoXGaze,12)
popDfPonzoLongUpX <- sample(popDfPonzoXGaze,12)
mean(popDfPonzoLongUpX) - mean(popDfPonzoControlsX)#random sample;get different random sample of 12;random variable of random sample
popDfPonzoControlsY <- sample(popDfPonzoYGaze,12)
popDfPonzoLongUpY <- sample(popDfPonzoYGaze,12)
mean(popDfPonzoLongUpY) - mean(popDfPonzoControlsY)
```

###### ^1^ Note: if knowing the null distribution, one can describe the proportion of values one sees for any interval of values 
###### ^1^ Note: define a number of times to redo the null hypothesis check; record all differences

```{r}
n<-10000
nullsPopX <- vector("numeric",n)
for(i in 1:n){
  popDfPonzoControlsX <- sample(popDfPonzoXGaze,12)
  popDfPonzoLongUpX <- sample(popDfPonzoXGaze,12)
  nullsPopX[i]<- mean(popDfPonzoLongUpX) - mean(popDfPonzoControlsX)
}
max(nullsPopX)
hist(nullsPopX)
```
```{r}
n<-10000
nullsPopY <- vector("numeric",n)
for(i in 1:n){
  popDfPonzoControlsY <- sample(popDfPonzoYGaze,12)
  popDfPonzoLongUpY <- sample(popDfPonzoYGaze,12)
  nullsPopY[i]<- mean(popDfPonzoLongUpY) - mean(popDfPonzoControlsY)
}
max(nullsPopY)
hist(nullsPopY)
```

1.[nullHypothesis]

```{r}
sum(nullsPopX > obslongUpControlsX)/n #1 option: how often null values are bigger or not than observed values 
sum(nullsPopY > obslongUpControlsY)/n 
```

```{r}
mean(nullsPopX >obslongUpControlsX) #2 option: proportion of times the null is bigger than the observation 
mean(nullsPopY >obslongUpControlsY)
```

```{r}
mean(abs(nullsPopX) >obslongUpControlsX)#3 option: how often it is bigger in absolute 
mean(abs(nullsPopY) >obslongUpControlsY)
```

###### ^1^ Note: p value = the probability that an outcome from the null distribution is bigger than what one observed when the null hypothesis is true 

> library load 

```{r}
library(rafalib)
```

```{r}
mypar()
qqnorm(nullsPopX)
qqline(nullsPopX)
qqnorm(nullsPopY)
qqline(nullsPopY)
```
###### ^1^ Note: use normal approximation instead of accessing the population through changes of the sample 

> library load 

```{r}
library(dplyr)
```

1.[tTest]

```{r}
N <- length(controlsXGaze)
se <- sqrt(var(longUpXGaze)/N+ #square root of the variance of the sample estimate of the variance divided by N=sample size
               var(controlsXGaze)/N) #gives us standard error estimate
tstatX <- obslongUpControlsX/se
tstatX
library(dplyr)
N <- length(controlsYGaze)
se <- sqrt(var(longUpYGaze)/N+ 
             var(controlsYGaze)/N) 
tstatY <- obslongUpControlsY/se
tstatY
```

```{r}
1- pnorm(tstatX) #what proportion of normally distributed data, with means 0 and standard deviation 1,are lower than whatever value you put here
2*(1- pnorm(tstatX))
1- pnorm(tstatY) 
2*(1- pnorm(tstatY))
```

```{r}
n<-10000
nullsPopX <- vector("numeric",n)
for(i in 1:n){
  popDfPonzoControlsX <- sample(popDfPonzoXGaze,12)
  popDfPonzoLongUpX <- sample(popDfPonzoXGaze,12)
  nullsPopX[i]<- mean(popDfPonzoLongUpX) - mean(popDfPonzoControlsX)
}

n<-10000
nullsPopY <- vector("numeric",n)
for(i in 1:n){
  popDfPonzoControlsY <- sample(popDfPonzoYGaze,12)
  popDfPonzoLongUpY <- sample(popDfPonzoYGaze,12)
  nullsPopY[i]<- mean(popDfPonzoLongUpY) - mean(popDfPonzoControlsY)
}
```

###### ^1^ Note: how good of an approximation this is => access the population 

> library load 

```{r}
library(rafalib)
```

```{r}
mypar()
qqnorm(nullsPopX)
abline(0,1)
qqline(nullsPopX)
qqnorm(nullsPopY)
abline(0,1)
qqline(nullsPopY)
```


## III.

1.[vector] discrete numeric data  considered ordinal-ly 

```{r}
unique(dfAlc$employrt)
length(unique(dfAlc$employrt))
```

1.[vector] compute the frequencies of each unique value 

```{r}
table(dfAlc$employrt)
prop.table(table(dfAlc$region))
```

1.[vector] sum frequencies 

```{r}
sum(table(dfAlc$employrt)==1)
```

> load library

```{r}
library(dplyr)
```

1.[column] remove NAs 

```{r}
dfAlc[is.na(x = dfAlc)] <- 0
```

1.[CDF] cumulative distribution function for continuous data 

```{r}
rangeForCdfFunction <- seq(min(dfAlc$employrt), max(dfAlc$employrt),length = 100) #define range of values spanning the dataset 
cdfFunction <- function(f){ # computes probability for a single value
  mean(dfAlc$employrt<=f) # cdfFunction (rangeForCdfFunction) = Pr (f</=rangeForCdfFunction) 
}
cdfValuesBelow <- sapply(rangeForCdfFunction,cdfFunction)#  CDF defines proportion of data below the  cutoff rangeForCdfFunction
cdfValuesAbove <- 1-(sapply(rangeForCdfFunction,cdfFunction)) # defines proportion of data above the  cutoff rangeForCdfFunction => 1 - cdfFunction (rangeForCdfFunction)
```

3.[plot] CDF 

```{r}
plot(rangeForCdfFunction,cdfValuesBelow)
EmployeeRate <- rangeForCdfFunction
Proportion<- cdfValuesBelow
plot(EmployeeRate,Proportion)
plot(rangeForCdfFunction,cdfValuesAbove) 
```

```{r}
rangeForCdfFunctionQ <- seq(quantile(dfAlc$employrt,0.50), quantile(dfAlc$employrt,0.75),length = 100) 
cdfFunction <- function(f){ 
  mean(dfAlc$employrt<=f)  
}
cdfValuesBellowQ <- sapply(rangeForCdfFunctionQ,cdfFunction) - (sapply(rangeForCdfFunction,cdfFunction)) # define proportion of values between rangeForCdfFunction and rangeForCdfFunctionQ
plot(rangeForCdfFunctionQ,cdfValuesBellowQ)

```

1.[column] mean and standard deviation 

```{r}
meanDfAlcEmpRate <- sum(dfAlc$employrt)/ length(dfAlc$employrt)
sdDfAlcEmpRate <- sqrt(sum((dfAlc$employrt-meanDfAlcEmpRate)^2)/ length(dfAlc$employrt))
```

1.[column] filter; access through index; calculate mean and standard deviation 

```{r}
iEMEA <- dfAlc$region == "EMEA" 
xEMEAEmplRt <- dfAlc$employrt[iEMEA]
mEMEAEmplRt <- mean(xEMEAEmplRt)
sdEMEAEmplRt <- sd(xEMEAEmplRt)
c(mEMEAEmplRt=mEMEAEmplRt,sdEMEAEmplRt=sdEMEAEmplRt)
```

1.[mean] within range 

```{r}
mean(xEMEAEmplRt>49 & xEMEAEmplRt<=52)
```

1.[zScore] scale

```{r}
zEMEAEmplRt = scale(xEMEAEmplRt) # converts a vector of approximatley normally distributed values into z-scores
```

1.[zScore] check whether significantly above or below the mean  

```{r}
mean(abs(zEMEAEmplRt)<2)  # compute proportion of observations that are within 2 standard deviations of the mean 
mean(abs(xEMEAEmplRt)<2) 
```

1[median] absolute deviation
 
```{r}
mad(xEMEAEmplRt) # median absolute deviation
```

1.[pnorm] approximate the proportion of the data without access to actual data 

```{r}
mean(xEMEAEmplRt>49 & xEMEAEmplRt<=52)  
pnorm(72, mean(xEMEAEmplRt),sd(xEMEAEmplRt))- pnorm(69,  mean(xEMEAEmplRt),sd(xEMEAEmplRt)) 
```

###### ^1^ Note:the approximation calculated with the pnorm is very close to the exact calculation with the mean 

1.[pnorm] approximation is not always useful  for the more extreme values - tails

```{r}
exact <- mean(xEMEAEmplRt > 21 & xEMEAEmplRt <= 23) # calculate the proportion of heights between 79 and 81
avg <- mean(xEMEAEmplRt)
sd <- sd(xEMEAEmplRt)
approx <- pnorm(81, avg, sd) - pnorm(79, avg, sd)# estimate the proportion of heights between 79 and 81 
exact/approx # report how many times bigger the actual proportion is compared to the approximation
```


1.[pnorm] mathematically define CDF

```{r}
1 - pnorm(45, mean(xEMEAEmplRt),sd(xEMEAEmplRt)) #whether the probability that a randomly selected employRt is bigger than 45
```

1.[plot] proportion table

```{r}
plot(prop.table(table(xEMEAEmplRt)), xlab = "a = EMEAEmplRt", ylab = "Pr(xEMEAEmplRt = a)")
```

1.[vector] quantile

```{r}
p <- seq(0.01, 0.99, 0.01)
quantile(p)
```

1.[vector] percentiles 

```{r}
percentiles <- quantile(xEMEAEmplRt,p)
percentiles[names(percentiles) == "25%"]
percentiles[names(percentiles) == "50%"]
percentiles[names(percentiles) == "75%"]
```

1.[vector] quartile

```{r}
summary(xEMEAEmplRt)
```

1.[vector] theoretical quantile

```{r}
p <- seq(0.01, 0.99, 0.01)
qnorm(p, mean(xEMEAEmplRt), sd(xEMEAEmplRt))
```

1.[vector] observed and theoretical quantile

```{r}
summary(xEMEAEmplRt)
mean(xEMEAEmplRt <= 54)# proportion of data below 68.5
p <- seq(0.05,0.95,0.05)
observedQuantiles <- quantile(xEMEAEmplRt,p)
theoreticalQuantiles <- qnorm(p, mean = mean(xEMEAEmplRt), sd = sd(xEMEAEmplRt))
```

1.[plot] observed and theoretical quantile

```{r}
plot(theoreticalQuantiles, observedQuantiles)
abline(0,1)
```

1.[vector] observed and theoretical quantile

```{r}
z <- scale(xEMEAEmplRt)
observedQuantiles <- quantile(z,p)# if standard units are used no need of defining the mean and the standard deviation 
theoreticalQuantiles <- qnorm(p) 
```

1.[plot] observed and theoretical quantile

```{r}
plot(theoreticalQuantiles, observedQuantiles)
abline(0,1)
```

> load library 

```{r}
library(dslabs)
```

1.[vector] filter

```{r}
iEMEA <- dfAlc$region == "EMEA"
iAPAC <- dfAlc$region == "APAC"
xEMEAEmplRt <- dfAlc$employrt[iEMEA]
xAPACEmplRt <- dfAlc$employrt[iAPAC]
```

1.[dataFrame] filter, group by, summarize

```{r}
  dfAlc %>%
  group_by(region) %>%
  dplyr::summarize(average = mean(aconsum), standard_deviation = sd(aconsum))
```

###### ^1^ Note: group data frame = as many tables with the same columns but not necessarily the same rows that are stacked together into one object
###### ^1^ Note: group_by() function from dplyr  converts a data frame to a grouped data frame, creating groups using one or more variables

1.[dataFrame] arrange by ascend 

```{r}
  dfAlc %>% arrange(aconsum) %>% head()
```

1.[dataFrame] arrange by descend 

```{r}
 dfAlc %>% arrange(desc(aconsum)) %>% head()
```

1.[dataFrame] nested arrange  

```{r}
 dfAlc %>% arrange(region, aconsum) %>% head()
```

1.[dataFrame] top 10, not ordered by 

```{r}
 dfAlc %>% top_n(10, aconsum)
```

1.[dataFrame] top 10,ordered by 

```{r}
dfAlc %>% arrange(desc(aconsum)) %>% top_n(10)
dfAlc %>% slice_max(aconsum, n = 10) #alternative 
```

1.[dataFrame]  top 10 nested arrange by region ,ordered by 

```{r}
dfAlc %>% arrange(desc(region, aconsum))  %>% top_n(10) 
```

1.[quantile] observed and theoretical calculate

```{r}
percentilesEMEA <- quantile(xEMEAEmplRt, seq(.01, 0.99, 0.01))
percentilesAPAC <- quantile(xAPACEmplRt, seq(.01, 0.99, 0.01))
theoreticalQuantilesEMEA <- qnorm(seq(.01, 0.99, 0.01), mean = mean(xAPACEmplRt), sd = sd(xAPACEmplRt))
theoreticalQuantilesAPAC <- qnorm(seq(.01, 0.99, 0.01), mean = mean(xAPACEmplRt), sd = sd(xAPACEmplRt))
```

1.[percentile] calculate

```{r}
EMEApercentiles <- c(percentilesEMEA[names(percentilesEMEA) == "10%"],percentilesEMEA[names(percentilesEMEA) == "30%"],percentilesEMEA[names(percentilesEMEA) == "50%"],percentilesEMEA[names(percentilesEMEA) == "70%"],percentilesEMEA[names(percentilesEMEA) == "90%"])
APACpercentiles <- c(percentilesAPAC[names(percentilesAPAC) == "10%"],percentilesAPAC[names(percentilesAPAC) == "30%"],percentilesAPAC[names(percentilesAPAC) == "50%"],percentilesAPAC[names(percentilesAPAC) == "70%"],percentilesAPAC[names(percentilesAPAC) == "90%"])

```

3.[plot] observed and theoretical quantiles calculate

```{r}
plot(theoreticalQuantilesEMEA, percentilesEMEA)
abline(0,1)
plot(theoreticalQuantilesAPAC, percentilesAPAC)
abline(0,1)
```

1.[zScore] scale

```{r}
zEMEA <- scale(xEMEAEmplRt)
zAPAC <- scale(xAPACEmplRt)
```

1.[quantile] observed and theoretical calculate

```{r}
percentilesEMEA <- quantile(zEMEA, seq(.01, 0.99, 0.01))
percentilesAPAC <- quantile(zAPAC, seq(.01, 0.99, 0.01))
theoreticalQuantilesEMEA <- qnorm(seq(.01, 0.99, 0.01))
theoreticalQuantilesAPAC <- qnorm(seq(.01, 0.99, 0.01))
```

3.[plot] observed and theoretical quantiles calculate

```{r}
plot(theoreticalQuantilesEMEA, percentilesEMEA)
abline(0,1)
plot(theoreticalQuantilesAPAC, percentilesAPAC)
abline(0,1)
```

1.[outlier] check error 

```{r}
     error_avg <- function(k){
    dfAlc$employrt[1]<-k
    mean(dfAlc$employrt)
  }
error_avg (10000)
error_avg (-10000)
  
```

1.[ggplot] define

```{r}
dfAlc %>% ggplot
ggplotdfAlc <- ggplot(data=dfAlc) 
class(ggplotdfAlc) 
print(ggplotdfAlc)
```

1.[ggplot] define variables and observations 

```{r}
head(dfAlc)
dfAlc %>% 
  filter(region == "EMEA")%>% 
  ggplot() +
  geom_point(aes(x = employrt, y = aconsum ),show.legend = TRUE)
dfAlc %>% 
  filter(region == "EMEA")%>% 
  ggplot() +
  geom_point(aes(x = urbanrt, y = aconsum ),show.legend = TRUE)
dfAlc %>% 
   filter(region == "EMEA")%>% 
  ggplot() +
  geom_point(aes(x = suicideper100, y = aconsum ),show.legend = TRUE)
```

1.[ggplot] plot defined

```{r}
gridExtra::grid.arrange(dfAlc %>% 
                           filter(region == "EMEA")%>% 
                          ggplot() +
                          geom_point(aes(x = employrt, y = aconsum ),show.legend = TRUE),
                        dfAlc %>% 
                           filter(region == "EMEA")%>% 
                          ggplot() +
                          geom_point(aes(x = urbanrt, y = aconsum ),show.legend = TRUE),
                        dfAlc %>% 
                           filter(region == "EMEA")%>% 
                          ggplot() +
                          geom_point(aes(x = suicideper100, y = aconsum ),show.legend = TRUE),
                        ncol = 3)
```

1.[ggplot] label text

```{r}
dfAlc %>% 
   filter(region == "EMEA")%>% 
  ggplot() +
  geom_point(aes(x = employrt, y = aconsum ),show.legend = TRUE) +
  geom_text(aes(x = employrt, y = aconsum, label = abbrv ))
dfAlc %>% 
   filter(region == "EMEA")%>% 
  ggplot() +
  geom_point(aes(x = urbanrt, y = aconsum ),show.legend = TRUE)+
  geom_text(aes(x = urbanrt, y = aconsum, label = abbrv ))
dfAlc %>% 
   filter(region == "EMEA")%>% 
  ggplot() +
  geom_point(aes(x = suicideper100, y = aconsum ),show.legend = TRUE)+
  geom_text(aes(x = suicideper100, y = aconsum, label = abbrv ))
```

1.[ggplot] plot label text

```{r}
dfAlc %>% 
   filter(region == "EMEA")%>% 
  ggplot(aes(x = employrt, y = aconsum, label = abbrv )) +
  geom_label()
dfAlc %>% 
   filter(region == "EMEA")%>% 
  ggplot(aes(x = urbanrt, y = aconsum, label = abbrv )) +
  geom_label()
dfAlc %>% 
   filter(region == "EMEA")%>% 
  ggplot(aes(x = suicideper100, y = aconsum, label = abbrv )) +
  geom_label()
```

1.[ggplot] change point size 

```{r}
dfAlc %>% 
   filter(region == "EMEA")%>% 
  ggplot() +
  geom_point(aes(x = employrt, y = aconsum ),show.legend = TRUE ,size = 0.6) +
  geom_text(aes(x = employrt, y = aconsum, label = abbrv ))
dfAlc %>% 
   filter(region == "EMEA")%>% 
  ggplot() +
  geom_point(aes(x = urbanrt, y = aconsum ),show.legend = TRUE,size = 0.6)+
  geom_text(aes(x = urbanrt, y = aconsum, label = abbrv ))
dfAlc %>% 
   filter(region == "EMEA")%>% 
  ggplot() +
  geom_point(aes(x = suicideper100, y = aconsum ),show.legend = TRUE,size = 0.6)+
  geom_text(aes(x = suicideper100, y = aconsum, label = abbrv ))
```

1.[ggplot] change point size relative to the frequency of the observations 

```{r}
dfAlc %>% 
  filter(region == "EMEA")%>% 
  ggplot() +
  geom_point(aes(x = employrt, y = aconsum,size = aconsum ),alpha= 0.5,show.legend = TRUE) +
  geom_text(aes(x = employrt, y = aconsum, label = abbrv ))
dfAlc %>% 
   filter(region == "EMEA")%>% 
  ggplot() +
  geom_point(aes(x = urbanrt, y = aconsum,size = aconsum ),alpha= 0.5,show.legend = TRUE)+
  geom_text(aes(x = urbanrt, y = aconsum, label = abbrv ))
dfAlc %>% 
  filter(region == "EMEA")%>% 
  ggplot() +
  geom_point(aes(x = suicideper100, y = aconsum,size = aconsum ),alpha= 0.5,show.legend = TRUE)+
  geom_text(aes(x = suicideper100, y = aconsum, label = abbrv ))
```

1.[ggplot] bubblechart 

```{r}
dfAlc %>% 
  filter(region == "EMEA")%>% 
  ggplot() +
  geom_point(aes(x = employrt, y = aconsum,size = urbanrt,color = suicideper100 ),alpha=0.5) +
  scale_size(range = c(.5, 20), name="urbanrt")+
  ggrepel::geom_text_repel(
    aes(x = employrt, y = aconsum, label = abbrv, point.size = suicideper100), 
    size = 4, # font size in the text labels
    point.padding = 0, #  padding around each point
    min.segment.length = 0, # draw all line segments
    box.padding = 0.7, #padding around each text label
    max.overlaps = 100
  ) +
  theme(legend.position = "right")
```


1.[ggplot] label position relative to point 

```{r}
dfAlc %>% 
  filter(region == "EMEA")%>% 
  ggplot() +
  geom_point(aes(x = employrt, y = aconsum ),show.legend = TRUE ,size = 0.6) +
  geom_text(aes(x = employrt, y = aconsum, label = abbrv ), nudge_x = 1 ,nudge_y = 0.5)
dfAlc %>% 
  filter(region == "EMEA")%>% 
  ggplot() +
  geom_point(aes(x = urbanrt, y = aconsum ),show.legend = TRUE ,size = 0.6)+
  geom_text(aes(x = urbanrt, y = aconsum, label = abbrv ),nudge_x = 1 ,nudge_y = 0.5)
dfAlc %>% 
  filter(region == "EMEA")%>% 
  ggplot() +
  geom_point(aes(x = suicideper100, y = aconsum ),show.legend = TRUE ,size = 0.6)+
  geom_text(aes(x = suicideper100, y = aconsum, label = abbrv ),nudge_x = 1 ,nudge_y = 0.5)
```

1.[ggplot] global aesthetic mapping 

```{r}
ggplotDfAlcAlconsumEmplRt <- dfAlc %>% filter(region == "EMEA")%>% ggplot(aes(employrt,aconsum,label = abbrv))
ggplotDfAlcAlconsumEmplRt + 
  geom_point(size=0.6)+
  geom_text(nudge_x = 1 ,nudge_y = 0.5)
ggplotDfAlcAlconsumUrbanRt <- dfAlc %>% filter(region == "EMEA")%>% ggplot(aes(urbanrt,aconsum,label = abbrv))
ggplotDfAlcAlconsumUrbanRt +
  geom_point(size=0.6)+
  geom_text(nudge_x = 1 ,nudge_y = 0.5)
ggplotDfAlcAlconsumSuicide100<- dfAlc %>% filter(region == "EMEA")%>% ggplot(aes(suicideper100,aconsum,label = abbrv))
ggplotDfAlcAlconsumSuicide100 +
  geom_point(size=0.6)+
  geom_text(nudge_x = 1 ,nudge_y = 0.5)
```

1.[ggplot] scale 

```{r}
dfAlc %>%
  filter(region == "EMEA")%>% 
  ggplot(aes(log2(aconsum))) +
  geom_histogram(binwidth = 1, fill ="lemonchiffon3" ,color = "lemonchiffon") 
dfAlc %>%
  filter(region == "EMEA")%>% 
  ggplot(aes(log2(employrt))) +
  geom_histogram(binwidth = 1, fill ="lightpink3" ,color = "lightpink") 
dfAlc %>%
  filter(region == "EMEA")%>% 
  ggplot(aes(log2(urbanrt))) +
  geom_histogram(binwidth = 1, fill = "lightgoldenrod3",color = "lightgoldenrod")
dfAlc %>%
  filter(region == "EMEA")%>% 
  ggplot(aes(log2(suicideper100))) +
  geom_histogram(binwidth = 1, fill = "lightblue3",color = "lightblue")
```

1.[ggplot] scale

```{r}
dfAlc %>%
  filter(region == "EMEA")%>% 
  ggplot(aes(aconsum)) +
  geom_histogram(binwidth = 1, fill ="lemonchiffon3" ,color = "lemonchiffon") +
 scale_x_continuous(trans = "log2")
dfAlc %>%
  filter(region == "EMEA")%>% 
  ggplot(aes(employrt)) +
  geom_histogram(binwidth = 1, fill ="lightpink3" ,color = "lightpink") +
  scale_x_continuous(trans = "log2")
dfAlc %>%
  filter(region == "EMEA")%>% 
  ggplot(aes(urbanrt)) +
  geom_histogram(binwidth = 1, fill = "lightgoldenrod3",color = "lightgoldenrod")+
  scale_x_continuous(trans = "log2")
dfAlc %>%
  filter(region == "EMEA")%>% 
  ggplot(aes(suicideper100)) +
  geom_histogram(binwidth = 1, fill = "lightblue3",color = "lightblue")+
  scale_x_continuous(trans = "log2")
```

1.[ggplot] change legend text and add title text

```{r}
ggplotDfAlcAlconsumEmplRt + 
  geom_point(size=0.6)+
  geom_text(nudge_x = 1 ,nudge_y = 0.5)+
  ylab("Alcohol Consumption Rate")+
  xlab("Employment Rate")+
  ggtitle("EMEA Alcohol Consumption")
ggplotDfAlcAlconsumUrbanRt +
  geom_point(size=0.6)+
  geom_text(nudge_x = 1 ,nudge_y = 0.5)+
  ylab("Alcohol Consumption Rate")+
  xlab("Urban Rate")+
  ggtitle("EMEA Alcohol Consumption")
ggplotDfAlcAlconsumSuicide100 +
  geom_point(size=0.6)+
  geom_text(nudge_x = 1 ,nudge_y = 0.5)+
  ylab("Alcohol Consumption Rate")+
  xlab("Suicide/100 People")+
  ggtitle("EMEA Alcohol Consumption")
```

1.[ggplot] color 

```{r}
ggplotDfAlcAlconsumEmplRt + 
  geom_point(size=0.6,color ="red3")+
  geom_text(nudge_x = 1 ,nudge_y = 0.5)+
  ylab("Alcohol Consumption Rate")+
  xlab("Employment Rate")+
  ggtitle("EMEA Alcohol Consumption")
ggplotDfAlcAlconsumUrbanRt +
  geom_point(size=0.6,color ="royalblue1")+
  geom_text(nudge_x = 1 ,nudge_y = 0.5)+
  ylab("Alcohol Consumption Rate")+
  xlab("Urban Rate")+
  ggtitle("EMEA Alcohol Consumption")
ggplotDfAlcAlconsumSuicide100 +
  geom_point(size=0.6,color ="limegreen")+
  geom_text(nudge_x = 1 ,nudge_y = 0.5)+
  ylab("Alcohol Consumption Rate")+
  xlab("Suicide/ 100 People")+
  ggtitle("EMEA Alcohol Consumption")
```
1.[ggplot] color to category 

```{r}
ggplotDfAlcAlconsumEmplRt + 
  geom_point(aes(col=region),size=0.6)+
  geom_text(nudge_x = 1 ,nudge_y = 0.5)+
  ylab("Alcohol Consumption Rate")+
  xlab("Employment Rate")+
  ggtitle("EMEA Alcohol Consumption")
ggplotDfAlcAlconsumUrbanRt +
  geom_point(aes(col=region),size=0.6)+
  geom_text(nudge_x = 1 ,nudge_y = 0.5)+
  ylab("Alcohol Consumption Rate")+
  xlab("Urban Rate")+
  ggtitle("EMEA Alcohol Consumption")
ggplotDfAlcAlconsumSuicide100 +
  geom_point(aes(col=region),size=0.6)+
  geom_text(nudge_x = 1 ,nudge_y = 0.5)+
  ylab("Alcohol Consumption Rate")+
  xlab("Suicide/ 100 People")+
  ggtitle("EMEA Alcohol Consumption")
```
1.[ggplot] color to category 

```{r}
dfAlc %>% 
  ggplot(aes(x = employrt, y = aconsum, label = abbrv, color = region )) +
  geom_label() +
  geom_text(check_overlap = TRUE)
dfAlc %>% 
  ggplot(aes(x = urbanrt, y = aconsum, label = abbrv, color = region )) +
  geom_label() +
  geom_text(check_overlap = TRUE)
dfAlc %>% 
  ggplot(aes(x = suicideper100, y = aconsum, label = abbrv,color = region )) +
  geom_label()+
  geom_text(check_overlap = TRUE)

```

1.[ggplot] intercept extraction

```{r}
dfAlcEMEA <- dfAlc %>% filter(region == "EMEA")
lm.rAcEmp <- summary(lm(dfAlcEMEA$aconsum ~ dfAlcEMEA$employrt ))# intercept extraction 
lm.rAcUrb <- summary(lm(dfAlcEMEA$aconsum  ~ dfAlcEMEA$urbanrt)) # intercept extraction 
lm.rAcSu <- summary(lm(dfAlcEMEA$aconsum  ~ dfAlcEMEA$suicideper100)) # intercept extraction 

```

1.[plot] linear 

```{r}
plot(lm(dfAlcEMEA$aconsum ~ dfAlcEMEA$employrt  ))
plot(lm(dfAlcEMEA$aconsum  ~ dfAlcEMEA$urbanrt ))
plot(lm(dfAlcEMEA$aconsum  ~ dfAlcEMEA$suicideper100 ))
```

1.[ggplot] line representing the intercept 

```{r}
ggplotDfAlcAlconsumEmplRt + 
  geom_point(aes(col=region),size=0.6)+
  geom_text(nudge_x = 1 ,nudge_y = 0.5)+
  ylab("Alcohol Consumption Rate")+
  xlab("Employment Rate")+ 
  ggtitle("EMEA Alcohol Consumption")+
  geom_abline(na.rm = TRUE, intercept= 4.84388)
ggplotDfAlcAlconsumUrbanRt +
  geom_point(aes(col=region),size=0.6)+
  geom_text(nudge_x = 1 ,nudge_y = 0.5)+
  ylab("Alcohol Consumption Rate")+
  xlab("Urban Rate")+
  ggtitle("EMEA Alcohol Consumption")+
  geom_abline(na.rm = TRUE, intercept = 3.85179)
ggplotDfAlcAlconsumSuicide100 +
  geom_point(aes(col=region),size=0.6)+
  geom_text(nudge_x = 1 ,nudge_y = 0.5)+
  ylab("Alcohol Consumption Rate")+
  xlab("Suicide/ 100 People")+
  ggtitle("EMEA Alcohol Consumption")+
  geom_abline(na.rm = TRUE, intercept = 2.04562)
```
1.[ggplot] color line 

```{r}
ggplotDfAlcAlconsumEmplRt + 
  geom_point(aes(col=region),size=0.6)+
  geom_text(nudge_x = 1 ,nudge_y = 0.5)+
  ylab("Alcohol Consumption Rate")+
  xlab("Employment Rate")+
  ggtitle("EMEA Alcohol Consumption")+
  geom_abline(na.rm = TRUE, intercept= 4.84388,lty = 1, color = "red" )
ggplotDfAlcAlconsumUrbanRt +
  geom_point(aes(col=region),size=0.6)+
  geom_text(nudge_x = 1 ,nudge_y = 0.5)+
  ylab("Alcohol Consumption Rate")+
  xlab("Urban Rate")+
  ggtitle("EMEA Alcohol Consumption")+
  geom_abline(na.rm = TRUE, intercept = 3.85179,lty =2 , color = "red" )
ggplotDfAlcAlconsumSuicide100 +
  geom_point(aes(col=region),size=0.6)+
  geom_text(nudge_x = 1 ,nudge_y = 0.5)+
  ylab("Alcohol Consumption Rate")+
  xlab("Suicide/ 100 People")+
  ggtitle("EMEA Alcohol Consumption")+
  geom_abline(na.rm = TRUE, intercept = 2.04562,lty = 3, color = "red" )
```
>load library 

```{r}
library(dslabs)
```

1.[ggthemes] themes

```{r}
ggplotDfAlcAlconsumSuicide100 +
  ggthemes::theme_economist()+
  geom_point(aes(col=region),size=0.6)+
  geom_text(nudge_x = 1 ,nudge_y = 0.5)+
  ylab("Alcohol Consumption Rate")+
  xlab("Suicide/ 100 People")+
  ggtitle("EMEA Alcohol Consumption")+
  geom_abline(na.rm = TRUE, intercept = 2.04562,lty = 3, color = "red" )

ggplotDfAlcAlconsumSuicide100 +
  ggthemes::theme_fivethirtyeight()+
  geom_point(aes(col=region),size=0.6)+
  geom_text(nudge_x = 1 ,nudge_y = 0.5)+
  ylab("Alcohol Consumption Rate")+
  xlab("Suicide/ 100 People")+
  ggtitle("World Alcohol Consumption")+
  geom_abline(na.rm = TRUE, intercept = 2.04562,lty = 3, color = "red" )

```

1.[ggrepel] includes a geometry that adds labels ensuring that they don't fall on top of each other

```{r}
ggplotDfAlcAlconsumSuicide100 +
  ggthemes::theme_fivethirtyeight()+
  geom_point(aes(col=region),size=1)+
  ggrepel::geom_text_repel(
    nudge_x = .15,
    box.padding = 0.5,
    nudge_y = 1,
    segment.curvature = -0.1,
    segment.ncp = 3,
    segment.angle = 30,
    max.overlaps = 100) + 
    theme_bw() + 
    theme(legend.position = 'none')+
  ylab("Alcohol Consumption Rate")+
  xlab("Suicide/ 100 People")+
  ggtitle("EMEA Alcohol Consumption")+
  geom_abline(na.rm = TRUE, intercept = 2.04562,lty = 3, color = "red" )
```


1.[ggrepel] includes a geometry that adds labels ensuring that they don't fall on top of each other

```{r}
ggplotDfAlcAlconsumSuicide100 +
  ggthemes::theme_fivethirtyeight()+
  geom_point(aes(col=region,size = aconsum ))+
  ggrepel::geom_text_repel(
    nudge_x = .15,
    box.padding = 0.5,
    nudge_y = 1,
    segment.curvature = -0.1,
    segment.ncp = 3,
    segment.angle = 30,
    max.overlaps = 100) + 
  theme_bw() + 
  theme(legend.position = 'none')+
  ylab("Alcohol Consumption Rate")+
  xlab("Suicide/ 100 People")+
  ggtitle("EMEA Alcohol Consumption")+
  geom_abline(na.rm = TRUE, intercept = 2.04562,lty = 3, color = "red" )


```


1.[geomHistogrma] plot geom_histogram 

```{r}
dfAlcEMEA <- dfAlc %>%
filter(region == "EMEA") %>%
ggplot(aes(x = aconsum))
dfAlcEMEA + 
  geom_histogram()
```

1.[geomHistogrma] add bin width

```{r}
dfAlcEMEA + 
  geom_histogram(binwidth = 1)
```

1.[geomHistogrma] color

```{r}
dfAlcEMEA + 
  geom_histogram(binwidth = 1, fill = "cyan3", col = "cyan4") 
```

1.[geomHistogrma] add legend and title

```{r}
dfAlcEMEA + 
  geom_histogram(binwidth = 1, fill = "cyan3", col = "cyan4")+
  xlab("EMEA Alcohol Consumption")+
  ggtitle("Histogram")
 
```

1.[geomHistogrma] put together

```{r}
dfAlcEMEA <- dfAlc %>%
  filter(region == "EMEA") %>%
  ggplot(aes(x = aconsum))
dfAlcAPAC <- dfAlc %>%
  filter(region == "APAC") %>%
  ggplot(aes(x = aconsum))
histEMEA <- dfAlcEMEA + geom_histogram(binwidth = 1, fill = "aquamarine2", col = "aquamarine4")
histAPAC <- dfAlcAPAC + geom_histogram(binwidth = 1, fill = "aquamarine4", col = "aquamarine2")
gridExtra::grid.arrange(histEMEA, histAPAC, ncol = 2)
```

1.[geomDensity] plot geom_density 

```{r}
dfAlcEMEA + 
  geom_density()
```

1.[geomDensity] color

```{r}
dfAlcEMEA + 
  geom_density(fill = "bisque2", col = "bisque3") +
  xlab("EMEA Alcohol Consumption") +
  ggtitle("Density Plot")
```

1.[geomDensity] group by category 

```{r}
dfAlc %>%
  ggplot(aes(aconsum, group = region))+
  geom_density()
```

1.[geomDensity] color by category 

```{r}
dfAlc %>%
  ggplot(aes(aconsum, color = region))+
  geom_density()
```

1.[geomDensity] fill by category 

```{r}
dfAlc %>%
  ggplot(aes(aconsum, fill = region))+
  geom_density()
```

1.[geomDensity] alpha by category 

```{r}
dfAlc %>%
  ggplot(aes(aconsum, fill = region))+
  geom_density(alpha = 0.2)
```

1.[geomQQ] geom_qq plot

```{r}
dfAlcEMEA <- dfAlc %>%
filter(region == "EMEA") %>%
ggplot(aes(sample = aconsum))
dfAlcEMEA + 
  geom_qq(fill = "darkgoldenrod2", col = "darkgoldenrod3") +
  geom_qq_line(col = "darkgray") +
  xlab("EMEA Alcohol Consumption") +
  ggtitle("QQ Plot")
```

###### ^1^ Note: By default, the Q-Q plot is compared to the normal distribution with avrg = 0 and sd = 1
 
1.[geomQQ] geom_qq plot 

```{r}
params <- dfAlc %>%
  filter(region == "EMEA") %>%
  dplyr::summarize(mean = mean(aconsum), sd = sd(aconsum))
dfAlcEMEA +
  geom_qq(dparams = params, fill = "darkgoldenrod2", col = "darkgoldenrod3") +
  geom_qq_line(col = "darkgray") + 
  geom_abline(col = "brown3")+ #how well the normal approximation works #the points fall roughly on the line. This is because this data is approximately normal.
  xlab("EMEA Alcohol Consumption") +
  ggtitle("QQ Plot")
```

###### ^1^ Note: the Q-Q plot is compared against a normal distribution with same mean/sd as data

1.[geomQQ] geom_qq plot 

```{r}
dfAlc %>%
filter(region == "EMEA") %>%
  ggplot(aes(sample = scale(aconsum))) +
  geom_qq(fill = "darkgoldenrod2", col = "darkgoldenrod3") +
  geom_abline(col = "cornflowerblue")+
  xlab("EMEA Alcohol Consumption") +
  ggtitle("QQ Plot")
```

###### ^1^ Note: the Q-Q plot is compared against a standard normal distribution with same mean/sd as data

1.[dataFrame] filter

```{r}
gapminder %>%
  dplyr::filter(year == 2015 & country %in% c("Sri Lanka", "Turkey"))%>%
  dplyr::select(country, infant_mortality)
gapminder %>%
   dplyr::filter(year == 2015 & country %in% c("Malaysia", "Russia"))%>%
   dplyr::select(country,infant_mortality)
gapminder %>%
  dplyr:: filter(year == 2015 & country %in% c("Syria", "Turkey"))%>%
  dplyr:: select(country,infant_mortality)
```

1.[scatterPlot] ggplot

```{r}
ds_theme_set()    # set plot theme
filter(gapminder, year == 1962) %>%
  ggplot(aes(fertility, life_expectancy, color = continent)) +
  geom_point()
ds_theme_set()    # set plot theme
filter(gapminder, year == 2012) %>%
  ggplot(aes(fertility, life_expectancy, color = continent)) +
  geom_point()
```

1.[scatterPlot] ggplot, geom_point, facet_grid 2 variables

```{r}
filter(gapminder, year %in% c(1962, 2012)) %>% 
  ggplot(aes(fertility, life_expectancy, col = continent)) +
  geom_point() +
  facet_grid(continent ~ year)
```

1.[scatterPlot] ggplot, geom_point, facet_grid 1 variable

```{r}
filter(gapminder, year %in% c(1962, 2012)) %>% 
  ggplot(aes(fertility, life_expectancy, col = continent)) +
  geom_point() +
  facet_grid(. ~ year)
```

1.[scatterPlot] ggplot, geom_point, facet_grid 1 variable in motion 

```{r}
years <- c(1962, 1980, 1990, 2000, 2012)
continents <- c("Europe", "Asia")
gapminder %>%
  filter(year %in% years & continent %in% continents) %>%
  ggplot(aes(fertility, life_expectancy, col = continent)) +
  geom_point() +
  facet_wrap(~year)
```
1.[scatterPlot] ggplot, geom_point for time series 

```{r}
gapminder %>%
  filter(country == "Turkey") %>%
  ggplot(aes(year, fertility)) +
  geom_point(aes(colour = country))+
  xlim(1962,2012)+
  ylim(0,10)
gapminder %>%
  filter(country == "Sri Lanka") %>%
  ggplot(aes(year, fertility)) +
  geom_point(aes(colour = country))+
  xlim(1962,2012)+
  ylim(0,10)
gapminder %>%
  filter(country == "Syria") %>%
  ggplot(aes(year, fertility)) +
  geom_point(aes(colour = country))+
  xlim(1962,2012)+
  ylim(0,10)
```
1.[linePlot] ggplot, geom_line for time series 

```{r}
gapminder %>%
  filter(country == "Turkey") %>%
  ggplot(aes(year, fertility)) +
  geom_line(aes(colour = country))+
  xlim(1962,2012)+
  ylim(0,10)
gapminder %>%
  filter(country == "Sri Lanka") %>%
  ggplot(aes(year, fertility)) +
  geom_line(aes(colour = country))+
  xlim(1962,2012)+
  ylim(0,10)
gapminder %>%
  filter(country == "Syria") %>%
  ggplot(aes(year, fertility)) +
  geom_line(aes(colour = country))+
  xlim(1962,2012)+
  ylim(0,10)
```
1.[linePlot] ggplot, geom_line for multiple time series 

```{r}
gapminder %>% filter(country %in% c("Turkey","Sri Lanka","Syria")) %>%
  ggplot(aes(year, fertility, group = country)) +
  geom_line()+
  xlim(1962,2012)+
  ylim(0,10)
```

1.[linePlot] ggplot, geom_line for multiple time series color

```{r}
gapminder %>% filter(country %in% c("Turkey","Sri Lanka","Syria")) %>%
  ggplot(aes(year, fertility, col = country)) +
  geom_line()+
  xlim(1962,2012)+
  ylim(0,10)
```

1.[linePlot] ggplot, geom_line, geom_text for multiple time series color, label 

```{r}
countries <-  c("Turkey","Sri Lanka","Syria")
labels <- data.frame(country = countries, x = c(1964, 1965,1964), y = c(6.40,5.50,7.85))
gapminder %>% filter(country %in% c("Turkey","Sri Lanka","Syria")) %>%
  ggplot(aes(year, fertility, col = country)) +
  geom_line() +
  xlim(1962,2012)+
  ylim(0,10)+
  geom_text(data = labels, aes(x, y, label = country), size = 3) +
  theme(legend.position = "none")
```

1.[dataFrame] mutate

```{r}
gapminder <- gapminder %>%
  mutate(dollars_per_day = gdp/population/365)
```

1.[linePlot] ggplot, geom_histogram,color 

```{r}
year1962 <- 1962 
gapminder %>%
  filter(year == year1962 & !is.na(gdp)) %>%
  ggplot(aes(dollars_per_day)) +
  geom_histogram(binwidth = 1, fill = "lightgoldenrod1", color = "black")+
  xlim(0,150)+
  ylim(0,30)
year2011 <- 2011
gapminder %>%
  filter(year == year2011 & !is.na(gdp)) %>%
  ggplot(aes(dollars_per_day)) +
  geom_histogram(binwidth = 1, fill = "indianred1", color = "black") +
  xlim(0,150)+
  ylim(0,30)
```

1.[linePlot] ggplot, geom_histogram, log, color 

```{r}
year1962 <- 1962 
gapminder %>%
  filter(year == year1962 & !is.na(gdp)) %>%
  ggplot(aes(log2(dollars_per_day))) +
  geom_histogram(binwidth = 1, fill = "lightgoldenrod1", color = "black")+
  xlim(0,150)+
  ylim(0,30) 
year2011 <- 2011
gapminder %>%
  filter(year == year2011 & !is.na(gdp)) %>%
  ggplot(aes(log2(dollars_per_day))) +
  geom_histogram(binwidth = 1, fill = "indianred1", color = "black") +
  xlim(0,150)+
  ylim(0,30)
```
1.[linePlot] ggplot, geom_histogram, log, color 

```{r}
year1962 <- 1962 
gapminder %>%
  filter(year == year1962 & !is.na(gdp)) %>%
  ggplot(aes(dollars_per_day)) +
  geom_histogram(binwidth = 1, fill = "lightgoldenrod1", color = "black")+
  xlim(0,32)+
  ylim(0,30)+
  scale_x_continuous(trans = "log2")
year2011 <- 2011
gapminder %>%
  filter(year == year2011 & !is.na(gdp)) %>%
  ggplot(aes(log2(dollars_per_day))) +
  geom_histogram(binwidth = 1, fill = "indianred1", color = "black") +
  xlim(0,32)+
  ylim(0,30)+
  scale_x_continuous(trans = "log2")
```

1.[boxPlot] geom_boxplot, rotate labels through theme

```{r}
year1962 <- 1962 
gapminder %>%
  filter(year == year1962 & !is.na(gdp)) %>%
  ggplot(aes(region, dollars_per_day))+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
year2011 <- 2011 
gapminder %>%
  filter(year == year2011 & !is.na(gdp)) %>%
  ggplot(aes(region, dollars_per_day))+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

1.[column] stratify factor by level

```{r}
length(levels(gapminder$continent))
```

1.[column] check levels of a factor

```{r}
regionFac <- levels(gapminder$continent)
regionFac
```

1.[column] reorder factor's levels on a summary computed on a numeric vector

```{r}
value <- c(10, 11, 12, 6, 4)
facRegByValue <- reorder(regionFac, value, FUN = mean)
levels(facRegByValue)
```

1.[boxPlot] filter, mutate by reorder by median, ggplot, geom_boxplot, theme

```{r}
gapminder %>%
  filter(year == year1962 & !is.na(gdp)) %>%
  mutate(region = reorder(region, dollars_per_day, FUN = median)) %>%   
  ggplot(aes(region, dollars_per_day, fill = continent)) +    
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("")
gapminder %>%
  filter(year == year2011 & !is.na(gdp)) %>%
  mutate(region = reorder(region, dollars_per_day, FUN = median)) %>%   
  ggplot(aes(region, dollars_per_day, fill = continent)) +    
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("")
```
1.[boxPlot] filter, mutate by reorder by median, ggplot, geom_boxplot, theme, scale log2 

```{r}
gapminder %>%
  filter(year == year1962 & !is.na(gdp)) %>%
  mutate(region = reorder(region, dollars_per_day, FUN = median)) %>%   
  ggplot(aes(region, dollars_per_day, fill = continent)) +    
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("")+
  scale_y_continuous(trans = "log2")
gapminder %>%
  filter(year == year2011 & !is.na(gdp)) %>%
  mutate(region = reorder(region, dollars_per_day, FUN = median)) %>%   
  ggplot(aes(region, dollars_per_day, fill = continent)) +    
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("")+
  scale_y_continuous(trans = "log2")
```

1.[boxPlot] filter, mutate by reorder by median, ggplot, geom_boxplot, theme, scale log2, geom_point

```{r}
gapminder %>%
  filter(year == year1962 & !is.na(gdp)) %>%
  mutate(region = reorder(region, dollars_per_day, FUN = median)) %>%   
  ggplot(aes(region, dollars_per_day, fill = continent)) +    
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("")+
  scale_y_continuous(trans = "log2")+ 
  geom_point(show.legend = FALSE)
gapminder %>%
  filter(year == year2011 & !is.na(gdp)) %>%
  mutate(region = reorder(region, dollars_per_day, FUN = median)) %>%   
  ggplot(aes(region, dollars_per_day, fill = continent)) +    
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("")+
  scale_y_continuous(trans = "log2")+ 
  geom_point(show.legend = FALSE)
```
1.[vector] define

```{r}
west <- c("Western Europe", "Northern Europe", "Southern Europe", "Northern America", "Australia and New Zealand")
```

1.[histogram] filter, mutate by reorder by ifelse group, ggplot, geom_histogram,scale log2, facet grid

```{r}
gapminder %>%
  filter(year %in% c(year1962, year2011) & !is.na(gdp)) %>%
  mutate(group = ifelse(region %in% west, "West", "Developing")) %>%
  ggplot(aes(dollars_per_day)) +
  geom_histogram(binwidth = 1, color = "black") +
  scale_x_continuous(trans = "log2") +
  facet_grid(year ~ group)
```

1.[vectors] define filtering only with available data

```{r}
countryList1962 <- gapminder %>%
  filter(year == year1962 & !is.na(dollars_per_day)) %>% .$country
countryList2011 <- gapminder %>%
  filter(year == year2011 & !is.na(dollars_per_day)) %>% .$country
country19622011Intersect <- intersect(countryList1962, countryList2011)
```

1.[histogram] filter, mutate by reorder by ifelse group, ggplot, geom_histogram,scale log2, facet grid

```{r}
gapminder %>%
  filter(year %in% c(year1962, year2011) & country %in% country19622011Intersect) %>%    # keep only selected countries
  mutate(group = ifelse(region %in% west, "West", "Developing")) %>%
  ggplot(aes(dollars_per_day)) +
  geom_histogram(binwidth = 1, color = "black") +
  scale_x_continuous(trans = "log2") +
  facet_grid(year ~ group)
```

1.[boxplot] 

```{r}
gapminder %>%
  filter(year %in% c(year1962, year2011) & country %in% country19622011Intersect) %>%
  mutate(region = reorder(region, dollars_per_day, FUN = median)) %>%
  ggplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("") +
  scale_y_continuous(trans = "log2")+ 
  geom_boxplot(aes(region, dollars_per_day, fill = continent)) +
  facet_grid(year ~ .)
```

1.[boxplot] 

```{r}
gapminder %>%
  filter(year %in% c(year1962, year2011) & country %in% country19622011Intersect) %>%
  mutate(region = reorder(region, dollars_per_day, FUN = median)) %>%
  ggplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("") +
  scale_y_continuous(trans = "log2")+  
  geom_boxplot(aes(region, dollars_per_day, fill = factor(year)))
```

1.[densityPlot] smooth 

```{r}
gapminder %>%
  filter(year == year1962 & country %in% country19622011Intersect) %>%
  mutate(group = ifelse(region %in% west, "West", "Developing")) %>%
  ggplot(aes(dollars_per_day, y = ..count.., fill = group)) +
  scale_x_continuous(trans = "log2")+ 
  geom_density(alpha = 0.2, bw = 0.75) + # bw for smoothness 
  facet_grid(year ~ .)

```

###### ^1^ Note: To have the areas of the densities be proportional to the size of the groups,multiply the y-axis values by the size of the group

1.[case_when] add group as a factor

```{r}
gapminder<-gapminder %>%
  mutate(group = case_when(
    .$region %in% west ~ "West",
    .$region %in% c("Eastern Asia", "South-Eastern Asia") ~ "East Asia",
    .$region %in% c("Caribbean", "Central America", "South America") ~ "Latin America",
    .$continent == "Africa" & .$region != "Northern Africa" ~ "Sub-Saharan Africa",
    TRUE ~ "Others"))
gapminder %>%
  mutate(group = factor(group, levels = c("Others", "Latin America", "East Asia", "Sub-Saharan Africa", "West")))
```

1.[densityPlot] stacked

```{r}
gapminder %>%
  filter(year %in% c(year1962, year2011) & country %in% country19622011Intersect) %>%
  ggplot(aes(dollars_per_day, fill = group)) +
  scale_x_continuous(trans = "log2")+
  geom_density(alpha = 0.2, bw = 0.75, position = "stack") +
  facet_grid(year ~ .)
```

1.[densityPlot] weighted stacked

```{r}
gapminder %>%
  filter(year %in% c(year1962, year2011) & country %in% country19622011Intersect) %>%
  group_by(year) %>%
  mutate(weight = population/sum(population*2)) %>%
  ungroup() %>%
  ggplot(aes(dollars_per_day, fill = group, weight = weight)) +
  scale_x_continuous(trans = "log2") +
  geom_density(alpha = 0.2, bw = 0.75, position = "stack") + facet_grid(year ~ .)
```

###### ^1^ Note:  ecological fallacy = the almost perfect relationship between survival rates and income is only observed for the averages at the regional level

1.[case_when] add group as a factor

```{r}
gapminder <- gapminder %>%
  mutate(group = case_when(
    .$region %in% west ~ "The West",
    .$region %in% "Northern Africa" ~ "Northern Africa",
    .$region %in% c("Eastern Asia", "South-Eastern Asia") ~ "East Asia",
    .$region == "Southern Asia" ~ "Southern Asia",
    .$region %in% c("Central America", "South America", "Caribbean") ~ "Latin America",
    .$continent == "Africa" & .$region != "Northern Africa" ~ "Sub-Saharan Africa",
    .$region %in% c("Melanesia", "Micronesia", "Polynesia") ~ "Pacific Islands"))
```

1.[logitTransformation]

```{r}
surv_income <- gapminder %>%
  filter(year %in% year2011 & !is.na(gdp) & !is.na(infant_mortality) & !is.na(group)) %>%
  group_by(group) %>%
  summarise(income = sum(gdp)/sum(population)/365,
            infant_survival_rate = 1 - sum(infant_mortality/1000*population)/sum(population))
surv_income %>% arrange(income)
```

###### ^1^ Note: logit transformation = f of p equals the log of p divided by 1 minus p

1.[scatterPlot]

```{r}
surv_income %>% 
  ggplot(aes(income, infant_survival_rate, label = group, color = group)) +
  scale_x_continuous(trans = "log2", limit = c(0.25, 150)) +
  scale_y_continuous(trans = "logit", limit = c(0.875, .9981),
                     breaks = c(.85, .90, .95, .99, .995, .998)) +
  geom_label(size = 3, show.legend = FALSE)
```

1.[scatterPlot] aes

```{r}
gapminder %>% 
  filter( year == 2011,continent == 'Africa') %>%
  ggplot(aes(fertility, life_expectancy)) +
  geom_point()
```

1.[scatterPlot]  color category 

```{r}
gapminder %>% 
  filter(year ==2011 & continent == "Africa") %>%
  ggplot(aes(fertility, life_expectancy, color = region))+
  geom_point()
```

1.[dataFrame] conditioning 

```{r}
gapminder %>%
  dplyr::filter(year == 2011 & continent =="Africa" & fertility <=3 & life_expectancy >= 70)%>%
  dplyr::select(country,region)
```

1.[linePlot] time series  

```{r}
gapminder %>%
  filter(country %in% c("Vietnam","United States","Cambodia") & year>=1960 & year <=2010) %>%
  ggplot(aes(year, life_expectancy,color = country))+
  geom_line()
```

1.[linePlot] frequency 

```{r}
gapminder %>% 
  mutate(dollars_per_day = gdp/population/365)%>%
  filter(continent == "Africa", year == 2010,!is.na(gdp)) %>%
  ggplot(aes(dollars_per_day, y = ..count..))+
  geom_density()+
  scale_x_continuous(trans = "log2")
```

1.[densityPlot]

```{r}
gapminder %>%
  filter(continent == "Africa", year  %in% c(1970,2010),!is.na(gdp)) %>% mutate(dollars_per_day = gdp/population/365) %>%
  ggplot(aes(dollars_per_day, y =..count..)) +
  geom_density()+
  scale_x_continuous(trans="log2")+
  facet_grid(.~year)
```

1.[densityPlot] stack 

```{r}
gapminder %>%
  filter(continent == "Africa", year  %in% c(1970,2010),!is.na(gdp)) %>% 
  mutate(dollars_per_day = gdp/population/365) %>%
  ggplot(aes(dollars_per_day, y =..count.., fill = region)) +
  geom_density(bw = 0.5, position = "stack")+
  scale_x_continuous(trans="log2")+
  facet_grid(year~.)
```


1.[scatterPlot]

```{r}
gapminder %>%
  filter(continent == "Africa", year  == 2010,!is.na(gdp)) %>% 
  mutate(dollars_per_day = gdp/population/365)%>% 
  ggplot(aes(dollars_per_day,infant_mortality,color = region))+
  geom_point()
```

1.[scatterPlot] scale

```{r}
gapminder %>%
  filter(continent == "Africa", year  == 2010,!is.na(gdp)) %>% 
  mutate(dollars_per_day = gdp/population/365)%>% 
  ggplot(aes(dollars_per_day,infant_mortality,color = region))+
  geom_point()+ 
  scale_x_continuous(trans="log2")
```

1.[scatterPlot] label

```{r}
gapminder %>%
  filter(continent == "Africa", year  == 2010,!is.na(gdp)) %>% 
  mutate(dollars_per_day = gdp/population/365)%>% 
  ggplot(aes(dollars_per_day,infant_mortality,color = region, label = country))+
  geom_point()+ 
  scale_x_continuous(trans="log2")+
  geom_text()
```

1.[scatterPlot] facet

```{r}
gapminder %>%
  filter(continent == "Africa", year  %in% c(1970,2010),!is.na(gdp),!is.na(infant_mortality)) %>% 
  mutate(dollars_per_day = gdp/population/365) %>%
  ggplot(aes(dollars_per_day,infant_mortality, color = region, label = country)) +
  geom_point()+
  scale_x_continuous(trans="log2")+
  geom_text()+
  facet_grid(year~.)
```

1.[scatterPlot] jitter  - add a small random shift to each point 

```{r}
heights %>% 
  ggplot(aes(sex, height)) + 
  geom_jitter(width = 0.1, alpha = 0.2)+ 
  geom_boxplot(alpha= 0.5)
```

>load 

```{r}
library(dplyr)
library(ggplot2)
library(dslabs)
```

1.[vector] reorder

```{r}
usContDiseasesMeasles <- us_contagious_diseases %>%
  filter(year == 1967 & disease=="Measles" & 
           !is.na(population)) %>% 
  mutate(rate = count / population * 10000 * 52 / weeks_reporting)
state <- usContDiseasesMeasles$state 
rate <- usContDiseasesMeasles$count/(usContDiseasesMeasles$population/10000)*(52/usContDiseasesMeasles$weeks_reporting)
state <- reorder(state,rate, FUN = mean)
```

>load 

```{r}
library(dplyr)
library(ggplot2)
library(dslabs)
```

1.[barPlot] numeric order  

```{r}
data(us_contagious_diseases)
usContDiseasesMeasles <- us_contagious_diseases %>% 
  filter(year == 1967 & disease=="Measles" & count>0 & !is.na(population)) %>%
  mutate(rate = count / population * 10000 * 52 / weeks_reporting)
usContDiseasesMeasles %>% 
  mutate(state = reorder(state, rate, FUN = mean)) %>% 
  ggplot(aes(state, rate)) +
  geom_bar(stat="identity") +
  coord_flip()
#
```

1.[barPlot] numeric order

```{r}
gapminder %>%  
  filter(!is.na(fertility)) %>%
  mutate(region = reorder(region, fertility, FUN = median)) %>% 
  ggplot(aes(region, fertility)) +
  geom_bar(stat="identity") +
  coord_flip()
```

1.[boxplot] numeric order 

```{r}
murders %>% 
  mutate(rate = total/population*100000) %>%
  mutate(region = reorder(region, rate, FUN = median)) %>% 
  ggplot(aes(region,rate)) +
  geom_boxplot() + 
  geom_point()
```

>load 

```{r}
library(tidyverse)
library(dslabs) 
data(gapminder)
```

1.[slopePlot] 

```{r}
west <- c("Western Europe", "Northern Europe", "Southern Europe", "Northern America", "Australia and New Zealand")
gapminder20002015Slope <- gapminder %>%
  filter(year %in% c(2010, 2015) & region %in% west & !is.na(life_expectancy) & population > 10^7)
gapminder20002015Slope %>%
  mutate(location = ifelse(year == 2010, 1, 2),
         location = ifelse(year == 2015 & country %in% c("United Kingdom", "Portugal"),
                           location + 0.22, location),
         hjust = ifelse(year == 2010, 1, 0)) %>%
  mutate(year = as.factor(year)) %>%
  ggplot(aes(year, life_expectancy, group = country)) +
  geom_line(aes(color = country), show.legend = FALSE) +
  geom_text(aes(x = location, label = country, hjust = hjust), show.legend = FALSE) +
  xlab("") +
  ylab("Life Expectancy") 

```

>load 

```{r}
library(ggrepel)
```

1.[scatterPlot] Bland-Altman plot = Tukey Mean Difference plot

```{r}
west <- c("Western Europe", "Northern Europe", "Southern Europe", "Northern America", "Australia and New Zealand")
gapminder20002015Slope <- gapminder %>%
  filter(year %in% c(2010, 2015) & region %in% west & !is.na(life_expectancy) & population > 10^7)
gapminder20002015Slope %>%
  mutate(year = paste0("life_expectancy_", year)) %>%
  dplyr::select(country, year, life_expectancy) %>% spread(year, life_expectancy) %>%
  mutate(average = (life_expectancy_2015 + life_expectancy_2010)/2,
         difference = life_expectancy_2015 - life_expectancy_2010) %>%
  ggplot(aes(average, difference, label = country)) +
  geom_point() +
  geom_text_repel() +
  geom_abline(lty = 2) +
  xlab("Average of 2010 and 2015") +
  ylab("Difference between 2015 and 2010")
```

>load

```{r}
library(tidyverse)
library(dslabs)
```

1.[linePlot] intercept and time series  

```{r}
the_disease <- "Measles"
usContDisMeasles <- us_contagious_diseases %>%
  filter(!state %in% c("Hawaii", "Alaska") & disease == the_disease) %>%
  mutate(rate = count / population * 10000 * 52/weeks_reporting) %>%
  mutate(state = reorder(state, rate))
usContDisMeasles %>% 
  filter(state == "California" & !is.na(rate)) %>%
  ggplot(aes(year, rate)) +
  geom_line() +
  ylab("Cases per 10,000") +
  geom_vline(xintercept=1963, col = "blue") 
```
1.[tilePlot]  heat map

```{r}
usContDisMeasles %>% 
  ggplot(aes(year, state, fill=rate)) +
  geom_tile(color = "grey50") +
  scale_x_continuous(expand = c(0,0)) +
  scale_fill_gradientn(colors = RColorBrewer::brewer.pal(9, "Reds"), trans = "sqrt") +
  geom_vline(xintercept = 1963, col = "blue") +
  theme_minimal() + theme(panel.grid = element_blank()) +
  ggtitle(the_disease) +
  ylab("") +
  xlab("")
```

1.[linePlot]

```{r}
avg <- us_contagious_diseases %>%
  filter(disease == the_disease) %>% group_by(year) %>%
  summarise(us_rate = sum(count, na.rm = TRUE)/sum(population, na.rm = TRUE)*10000)
#  line plot of measles rate by year by state
usContDisMeasles %>%
  filter(!is.na(rate)) %>%
  ggplot() +
  geom_line(aes(year, rate, group = state), color = "grey50", 
            show.legend = FALSE, alpha = 0.2, size = 1) +
  geom_line(mapping = aes(year, us_rate), data = avg, size = 1, col = "black") +
  scale_y_continuous(trans = "sqrt", breaks = c(5, 25, 125, 300)) +
  ggtitle("Cases per 10,000 by state") +
  xlab("") +
  ylab("") +
  geom_text(data = data.frame(x = 1955, y = 50),
            mapping = aes(x, y, label = "US average"), color = "black") +
  geom_vline(xintercept = 1963, col = "blue")
```

>load

```{r}
library(dplyr)
library(ggplot2)
library(dslabs)
```

1.[tilePlot] heat map 

```{r}
the_disease = "Smallpox"
usContDiseasesSmallpox <- us_contagious_diseases %>% 
  filter(!state%in%c("Hawaii","Alaska") & disease == the_disease & weeks_reporting >= 10) %>% 
  mutate(rate = count / population * 10000) %>% 
  mutate(state = reorder(state, rate))
usContDiseasesSmallpox %>% 
  ggplot(aes(year, state, fill = rate)) + 
  geom_tile(color = "grey50") + 
  scale_x_continuous(expand=c(0,0)) + 
  scale_fill_gradientn(colors = brewer.pal(9, "Reds"), trans = "sqrt") + 
  theme_minimal() + 
  theme(panel.grid = element_blank()) + 
  ggtitle(the_disease) + 
  ylab("") + 
  xlab("")
```

1.[linePlot]

```{r}
the_disease = "Smallpox"
usContDiseasesSmallpox <- us_contagious_diseases %>%
  filter(!state%in%c("Hawaii","Alaska") & disease == the_disease & weeks_reporting >= 10) %>%
  mutate(rate = count / population * 10000) %>%
  mutate(state = reorder(state, rate))
avg <- us_contagious_diseases %>%
  filter(disease==the_disease & weeks_reporting >= 10) %>% group_by(year) %>%
  summarise(us_rate = sum(count, na.rm=TRUE)/sum(population, na.rm=TRUE)*10000)
usContDiseasesSmallpox %>% 
  ggplot() +
  geom_line(aes(year, rate, group = state),  color = "grey50", 
            show.legend = FALSE, alpha = 0.2, size = 1) +
  geom_line(mapping = aes(year, us_rate),  data = avg, size = 1, color = "black") +
  scale_y_continuous(trans = "sqrt", breaks = c(5,25,125,300)) + 
  ggtitle("Cases per 10,000 by state") + 
  xlab("") + 
  ylab("") +
  geom_text(data = data.frame(x=1955, y=50), mapping = aes(x, y, label="US average"), color="black") + 
  geom_vline(xintercept=1963, col = "blue")

```

>load

```{r}
library(dplyr)
library(ggplot2)
library(dslabs)
library(RColorBrewer)
```

1.[linePlot] 

```{r}
data(us_contagious_diseases)
us_contagious_diseases %>% 
  filter(state=="California"  & weeks_reporting >= 10) %>% 
  group_by(year, disease) %>%
  summarise(rate = sum(count)/sum(population)*10000) %>%
  ggplot(aes(year, rate,color = disease)) + 
  geom_line()
```
1.[linePlot]

```{r}
us_contagious_diseases %>%
  filter(!is.na(population)) %>%
  group_by(year, disease) %>%
  summarise(rate = sum(count)/sum(population)*10000) %>%
  ggplot(aes(year, rate, color = disease)) +
  geom_line()
```
>load 

```{r}
library(tidyverse)
library(titanic)
```

1.[length] reduce digits' number 

```{r}
options(digits = 3)  
```

1.[dataFrame] select mutate 

```{r}
titanic <- titanic_train %>%
  dplyr::select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare) %>%
  mutate(Survived = factor(Survived),
         Pclass = factor(Pclass),
         Sex = factor(Sex))
```

1.[densityPlot] facet

```{r}
titanic %>%
  ggplot(aes(Age, fill = Sex)) +
  geom_density(alpha = 0.2) +
  facet_grid(Sex ~ .)
```
1.[densityPlot] without facet 

```{r}
titanic %>%
  ggplot(aes(Age, fill = Sex)) +
  geom_density(alpha = 0.2)
```
1.[frequencyPlot]

```{r}
titanic %>%
  ggplot(aes(Age, y = ..count.., fill = Sex)) +
  geom_density(alpha = 0.2, position = "stack")
```

1.[QQPlot] 

```{r}
params <- titanic %>%
  filter(!is.na(Age)) %>%
  summarise(mean = mean(Age), sd = sd(Age))
titanic %>%
  ggplot(aes(sample = Age)) +
  geom_qq(dparams = params) +
  geom_abline()
```

1.[barPlot] 

```{r}
titanic %>%
  ggplot(aes(Survived, fill = Sex)) +
  geom_bar()
```
1.[barPlot] position_dodge

```{r}
titanic %>%
  ggplot(aes(Survived, fill = Sex)) +
  geom_bar(position = position_dodge())
```

1.[barPlot]

```{r}
titanic %>%
  ggplot(aes(Sex, fill = Survived)) +
  geom_bar()
```

1.[densityPlot]

```{r}
titanic %>%
  ggplot(aes(Age, y = ..count.., fill = Survived)) +
  geom_density(alpha = 0.2)
```
1.[boxplot] geom_jitter

```{r}
titanic %>%
  filter(Fare != 0)%>%
  ggplot(aes(Survived,Fare)) +
  geom_boxplot(alpha = 0.2) +
  scale_y_continuous(trans = "log2")+
  geom_jitter()
```

1.[barPlot]

```{r}
titanic %>%
  ggplot(aes(Pclass, fill = Survived))+
  geom_bar()
```
1.[barPlot] show relative proportions in each group instead of counts

```{r}
titanic %>%
  ggplot(aes(Pclass, fill = Survived))+
  geom_bar(position = position_fill())
```

1.[barPlot]

```{r}
titanic %>%
  ggplot(aes(Survived, fill = Pclass))+
  geom_bar(position = position_fill())
```

1.[desnityPlot] grid 

```{r}
titanic %>%
  ggplot(aes(Age, fill = Survived, y = ..count..)) +
  geom_density(alpha = 0.2) +
  facet_grid(Pclass~ Sex) 
```
>load

```{r}
library(tidyverse)
library(dslabs)
data(stars)
```

1.[digits] reduce number

```{r}
options(digits = 3)
```

1.[numbers] mean and sd

```{r}
mean(stars$magnitude)
sd(stars$magnitude)
```

1.[desnityPlot]

```{r}
stars %>%
  ggplot(aes(magnitude)) +
  geom_density(alpha = 0.2) 
```

1.[histogram] 

```{r}
stars %>% 
  ggplot(aes(temp)) +
  geom_histogram()
```
1.[scatterPlot]

```{r}
stars %>% 
  ggplot(aes(temp, magnitude)) +
  geom_point() #decreasing exponential
```

1.[logTransform]
 
```{r}
stars %>% 
  ggplot(aes(temp, magnitude)) +
  scale_y_reverse()+
  scale_x_continuous(trans = "log10")+
  scale_x_reverse()+
  geom_point()
```
1.[estimation] estimation of average 

```{r}
estimateGigants<- stars%>%
  filter(temp <10000)
mean(estimateGigants$temp)
```

1.[scatterPlot] geom_label 

```{r}
# label
stars %>% 
  ggplot(aes(temp, magnitude,label = star)) +
  scale_y_reverse()+
  scale_x_continuous(trans = "log10")+
  scale_x_reverse()+
  geom_label(size = 3, show.legend = TRUE) 
```

1.[scatterPlot] color point 

```{r}

head(stars)
stars %>%  
  ggplot(aes(temp, magnitude, color = type)) +
  scale_y_reverse()+
  scale_x_continuous(trans = "log10")+
  scale_x_reverse()+
  geom_point()
```

>load

```{r}
library(tidyverse)
library(dslabs)
data(temp_carbon)
data(greenhouse_gases)
data(historic_co2)
head(temp_carbon)
```
1.[vector] pull max 

```{r}
temp_carbon %>%
  dplyr::filter(!is.na(carbon_emissions)) %>%
  dplyr:: pull(year) %>%
  max()
#
temp_carbon %>%
  dplyr::filter(!is.na(carbon_emissions)) %>%
  .$year %>%
  max()
#
temp_carbon %>%
  dplyr::filter(!is.na(carbon_emissions)) %>%
  dplyr::select(year) %>%
  max()
#
temp_carbon %>%
  dplyr::filter(!is.na(temp_anomaly)) %>%
  dplyr::pull(year) %>%
  max()
```

1.[vector] pull min 

```{r}
temp_carbon %>%
  dplyr::filter(!is.na(carbon_emissions)) %>%
  dplyr::pull(year) %>%
  min()
temp_carbon %>%
  dplyr::filter(!is.na(temp_anomaly)) %>%
  dplyr::pull(year) %>%
  min()
```

1.[vector] proportion 

```{r}
lastYear <- temp_carbon %>%
  filter(year == 2014)
firstYear <- temp_carbon %>%
  filter(year == 1751)
proportion<- lastYear$carbon_emissions/firstYear$carbon_emissions

```

1.[vector] difference

```{r}
lastYear <- temp_carbon %>%
  filter(year == 2018)
firstYear <- temp_carbon %>%
  filter(year == 1880)
difference<- lastYear$temp_anomaly - firstYear$temp_anomaly
```

1.[vector] mean across interval 

```{r}
twCentury <- temp_carbon %>%
  filter(!is.na(temp_anomaly) & year %in% c(1901:2000))
```

1.[linePlot] 

```{r}
twCentury %>%
  ggplot(aes(year,temp_anomaly)) +
  geom_line() +
  geom_hline(yintercept = 0, col="blue") +
  ylab("t anomaly in C degrees") +
  ggtitle("t anomaly relative to 20th century mean, 1880-2018") +
  geom_text(aes(x = 2000, y = 0.05, label = "20th century mean"), col = "blue") +
  geom_vline(aes(xintercept=1939),col="red") +
  geom_vline(aes(xintercept=1976),col="green") +
  geom_text(aes(x=1936,y=0.05,label="1939"),col="red") +
  geom_text(aes(x=1978,y=0.05,label="1976"),col="green")
```

1.[linePlot]

```{r}
  temp_carbon%>%
    filter(year %in% c(1880:2020)) %>%
    filter(!is.na(temp_anomaly)) %>%
    filter(!is.na(ocean_anomaly))%>%
    filter(!is.na(land_anomaly))%>%
    ggplot() +
    geom_line(aes(x = year,y=temp_anomaly), col = "black")+
    geom_line(aes(x = year,y=ocean_anomaly), col = "blue")+
    geom_line(aes(x = year,y=land_anomaly), col = "green")+ 
    ylab("anomaly")+
    geom_hline(yintercept = 0, col="black",alpha=0.5) +
    geom_vline(xintercept = 2018, col="black",alpha=0.5)
```

1.[linePlot]

```{r}
greenhouse_gases %>%
  ggplot(aes(x= year,y = concentration)) +
  geom_line() +
  facet_grid(gas ~., scales = "free") +
  geom_vline(xintercept = 1850)+ #Add a vertical line with an x-intercept at the year 1850, noting the unofficial start of the industrial revolution and widespread fossil fuel consumption
  ylab("Concentration (ch4/n2o ppb, co2 ppm)") +
  ggtitle("Atmospheric greenhouse gas concentration by year, 0-2000")
```

1.[linePlot] time series 

```{r}
temp_carbon%>%
  filter(!is.na(carbon_emissions)) %>%
  ggplot() +
  geom_line(aes(x = year,y=carbon_emissions), col = "black")+
  ylab("carbon emissions")
```
1.[linePlot]

```{r}
historic_co2 %>%
  ggplot(aes(year,co2,color = source))+
  geom_line()
```

1.[linePlot]

```{r}
historic_co2 %>%
    filter(year>1500) %>%
    ggplot(aes(year,co2,color=source)) +
    geom_line()
```

1.{linePlot}

```{r}
historic_co2 %>%
  ggplot(aes(year,co2,color=source)) +
  geom_line() +
  xlim(-800000,-775000)
historic_co2 %>%
    ggplot(aes(year,co2,color=source)) +
    geom_line() +
    xlim(-375000 ,-330000)
historic_co2 %>%
  ggplot(aes(year,co2,color=source)) +
  geom_line() +
  xlim(-140000 ,-120000)
historic_co2 %>%
    ggplot(aes(year,co2,color=source)) +
    geom_line() +
    xlim(1700,2018)
```

# IV. Probability 

> load

```{r}
library(tidyverse)
library(dslabs)
data(heights)
```

```{r}
heightsM <- heights %>% 
  filter(sex=="Male") %>% 
  pull(height)
```

1.[CDf] cumulative distribution function for the normal distribution, theoretical  

```{r}
a <- 70
plea <- function(a) mean(heightsM <= a)
pha <- 1 - plea(a) 
pha
```

###### ^1^ Note: F(a) = Pr(x<=a); 1-F(a) = Pr(x>a) = 1 - Pr(x<=a)
###### ^1^ Note: F(a) = plea; 1-F(a) = pha
###### ^1^ Note: CDf operates on intervals rather than single values

1.[pnorm] integral from âˆ’âˆž to q of the PDnf of the normal distribution

>load

```{r}
library(tidyverse)
library(dslabs)
data(heights)
```

```{r}
heightsM <- heights %>% 
  filter(sex=="Male") %>% 
  pull(height)
```

1.[CDf] cumulative distribution function for the normal distribution

```{r}
a <- 70.5
1 - pnorm(a, mean(heightsM), sd(heightsM)) 
```

###### ^1^ Note:PDf calculates CDf F(a) = P(x <= a), where x is normal 
###### ^1^ Note: PDf probability distribution function integral of the PDnf probability density function
###### ^1^ Note: no need of data ->  just mean and sd 
###### ^1^ Note:  the normal distribution is defined for continuous variables; not described for discrete variables
###### ^1^ Note: a continuous variable can be taken as categorical, e.g. each specific variable as a unique category -> PD is then defined by the proportion of n - number of reporting each of those unique variables

1.[plot]PD

```{r}
plot(prop.table(table(heightsM)), xlab = "a = Height in inches", ylab = "Pr(x = a)")
```

1.[pnorm] interval of 1 

```{r}
mean(heightsM <= 68.5) - mean(heightsM <= 67.5)
mean(heightsM <= 69.5) - mean(heightsM <= 68.5)
mean(heightsM <= 70.5) - mean(heightsM <= 69.5)
```

```{r}
pnorm(68.5, mean(heightsM), sd(heightsM)) - pnorm(67.5, mean(heightsM), sd(heightsM)) 
pnorm(69.5, mean(heightsM), sd(heightsM)) - pnorm(68.5, mean(heightsM), sd(heightsM))
pnorm(70.5, mean(heightsM), sd(heightsM)) - pnorm(69.5, mean(heightsM), sd(heightsM))
```

###### ^1^ Note: Do not calculate p on actual data over ranges differing from 1 
###### ^1^ Note:  discretization = when true variable distribution is continuous and reported variable's tend to be more common at discrete values, in this case, due to rounding
###### ^1^ Note: PDf, f(x),quantity with the most similar interpretation to the p of a single value x
###### ^1^ Note: PDn = Pr(x)= freq of x/ N of X
###### ^1^ Note: CDf = if F(a) = Pr(x<=a), then Pr(x=1) + Pr(x=2) etc.
###### ^1^ Note: integral of f(x) over a range gives the CDf of that range 

1.[dnorm] density function,PDnf for the normal distribution 

```{r}
library(tidyverse)
x <- seq(-4, 4, length = 100)
data.frame(x, f = dnorm(x)) %>%
  ggplot(aes(x, f)) +
  geom_line()
```


###### ^1^ Note: essential to those wanting to fit models to data for which predefined functions are not available
###### ^1^ Note: dnorm(z) -> density curve (calculating the density over a range of possible values of z) for the normal distribution
###### ^1^ Note: p density f(z), which is dnorm() of the series of z-scores, plot z  against f(z)
###### ^1^ Note: dnorm(z, mu, sigma), an alternative normal distribution

1.[rnorm] random numbers from the normal distribution

>load

```{r}
library(tidyverse)
library(dslabs)
data(heights)
```

```{r}
heightsM <- heights %>% 
  filter(sex=="Male") %>% 
  pull(height)
n <- length(heightsM)
avg <- mean(heightsM)
s <- sd(heightsM)
simulatedHeights <- rnorm(n, avg, s) # generates simulated x data using normal distribution
data.frame(simulatedHeights = simulatedHeights) %>%
  ggplot(aes(simulatedHeights)) +
  geom_histogram(color="black", binwidth = 2) 
```
```{r}
B <- 10000  
tallest <- replicate(B, {
  simulatedData <- rnorm(800, avg, s)    # generate 800 normally distributed random x
  max(simulatedData)    # determine max x from 800 normally distributed random x with avg x and s x
}) 
mean(tallest >= 7*12)    # proportion of times that max x exceeded 7 feet (84 inches)
```


1.[PDf]

>load

```{r}
library(tidyverse)
library(dslabs)
data(heights)
```

```{r}
heightsF <- heights %>% 
  filter(sex=="Female") %>% 
  pull(height)
avg <- mean(heightsF)
sd <- sd(heightsF)
pnorm(5*12,avg,sd) # pick a female at random, check the p that height =< 5
1 - pnorm(6*12,avg,sd) #  pick a female at random, check the p that height >= 6
pnorm(67,avg,sd) - pnorm(61,avg,sd)# pick a female at random, check the p that height 61 ~ 67 
oneSdUp <- avg + sd # check within 1SD up from the average 
oneSdDown <- avg - sd # check within 1SD down from the average 
pnorm(oneSdUp,avg,sd) - pnorm(oneSdDown,avg,sd)#  p that x of a randomly chosen female is within 1SD from the average height
qnorm(0.99,avg,sd) # x of a female in the 99th percentile
```

1.[PDf]

```{r}
B <- 1000
l <- 10000
avg <- 100
sd <- 15
set.seed(1) 
highestIQ <- replicate(B, {
  simData <- rnorm(l, avg, sd)    
  max(simData)    
})
hist(highestIQ)
```

1.[PDf]

```{r}
l <-10000
avg <-20.9 
sd <- 5.7
set.seed(16,sample.kind = "Rounding")
actScr <- rnorm(l, avg , sd)
meanActScr <- mean(actScr) 
sdActScr <- sd(actScr)
```

```{r}
perfectActScr <-36
modestActScr<- 30
lowActScr<- 10
sumperfectActScr <-  sum(actScr>=perfectActScr) # n perfect scores  out of l = 10,000 simulated tests
meanmodestActScr <- mean(actScr>=modestActScr) # p of an ACT score <  or = to 30
meanlowActScr <- mean(actScr <= lowActScr)   # p of an ACT score <  or = to 10
sumperfectActScr
meanmodestActScr
meanlowActScr
```

1.[PDnf]determine

```{r}
xSeqScr <- seq(1, 36)
fxSeqScr <- dnorm(xSeqScr, avg, sd)
plot(xSeqScr, fxSeqScr)
```
1.[2SDam] determine p deviation 

```{r}
zActScr <- (actScr - meanActScr) / sdActScr
meanzActScr <- mean(zActScr > 2)
meanzActScr
zScore <- 2
zScore * sdActScr + meanActScr 
qnorm(.975, meanActScr, sdActScr) # determine the 97.5th percentile of normally distributed data
cdf <- sapply(1:36, function (a){ # takes value ->  produces the p of a score < or = to that value
  mean(actScr <= a) 
})
min(which(cdf >= .95))# min score such that the p of that score or lower is at least .95
qnorm(.95, 20.9, 5.7) # determine the expected 95th percentile, the value for which the p of receiving that score or lower is 0.95
p <- seq(0.01, 0.99, 0.01) # quantiles for p
sampleQuantiles <- quantile(actScr, p)
names(sampleQuantiles[max(which(sampleQuantiles < 26))]) # in what percentile is a score of 26
```

###### ^1^ Note:  z = 2  corresponds to 97.5th percentile

>load

```{r}
library(tidyverse)
library(ggplot2)
```

```{r}
p <- seq(0.01, 0.99, 0.01)
sampleQuantiles <- quantile(actScr, p)
theoretQuantiles <- qnorm(p, 20.9, 5.7) # a corresponding set of theoretical quantiles
qplot(theoretQuantiles, sampleQuantiles) + 
  geom_abline()
```

1.[RD] random variables' distribution 

```{r}
green <- 2
black <- 18
red <- 18
winamount <- 17
looseAmount <- -1
pInGreen = green/(green+black+red)
pNotInGreen <- 1 - pInGreen
set.seed(1)
X <- sample(c(winamount,looseAmount), 1, replace = TRUE, prob=c(pInGreen, pNotInGreen)) # the random variable `X`, one's winnings from betting on green
X
EX <- pInGreen * winamount + pNotInGreen * (looseAmount)
singleOutOneSpin <- abs((17 - -1))*sqrt(pInGreen*pNotInGreen) 
set.seed(1)
n <- 1000
S <- sum(X) # a random variable S that sums one's winnings after betting on green 1,000 times
S
EXn <- n * (pInGreen * winamount + pNotInGreen * (looseAmount))# E outcome of a bet
SEXn <- sqrt(n) * abs((winamount + 1))*sqrt(pInGreen*pNotInGreen)# SE of the sum of 1,000 outcomes

```


###### ^1^ Note: random variables resulting from random processes 
###### ^1^ Note: sampling models = = models the random behavior of a process
###### ^1^ Note: PD of a random variable = p of the observed value falling in any given interval
###### ^1^ Note: expected value = average of many draws of a random variable; E of X = mu
###### ^1^ Note: standard error = sd of many draws of a random variable; SE = the typical difference between a random variable and its expectation 
###### ^1^ Note: random variable has a distribution function  -> CLT
###### ^1^ Note: CLT -> when the n of independent draws (sample size)is large,the PD of the sum of these draws is approximately normal

1.[RD] random variables' distribution 

```{r}
pInGreen = green/(green+black+red)
pNotInGreen <- 1 - pInGreen
winamount <- 17
looseAmount <- -1
n <- 100
avg <- n * (winamount*pInGreen + looseAmount*pNotInGreen)
se <- sqrt(n) * (winamount - looseAmount)*sqrt(pInGreen*pNotInGreen)
1 - pnorm(0,avg,se)
```

###### ^1^ Note: the law of averages is sometimes misinterpreted -> assuming that draws of categorically different variables are dependent (heads vs. tails are independent;the chance of a coin landing heads is 50%, regardless of the previous landings)
###### ^1^ Note: the law of averages applies only when the number of draws is very, very large, not in small samples
###### ^1^ Note: if the p of success is very small, one needs larger sample sizes -> Poisson distribution is needed

1.[RD] random variables' distribution Monte Carlo

```{r}
pInGreen = green/(green+black+red)
pNotInGreen <- 1 - pInGreen
winamount <- 17
looseAmount <- -1
n <- 100
B <- 10000 
set.seed(1)
S <- replicate(B,{
  X <- sample(c(winamount,looseAmount), size = n, replace = TRUE, prob = c(pInGreen, pNotInGreen))
  sum(X)
})
mean(S)
sd(S)
```

1.[RD] random variables' distribution Monte Carlo vs. CLT

```{r}
pInGreen = green/(green+black+red)
pNotInGreen <- 1 - pInGreen
winamount <- 17
looseAmount <- -1
n <- 10000
set.seed(1)
X <- sample(c(winamount,looseAmount), size = n, replace = TRUE, prob = c(pInGreen, pNotInGreen))
Y <- mean(X)
Y # average winnings per bet
EY <- pInGreen * winamount + pNotInGreen * (looseAmount) # E of Y, the average outcome per bet 
EY
SEY <- abs((17 - (-1))*sqrt(pInGreen*pNotInGreen) / sqrt(n)) # SE of Y, the standard error outcome per bet 
SEY
avg <- winamount*pInGreen + looseAmount*pNotInGreen
se <- 1/sqrt(n) * (winamount - looseAmount)*sqrt(pInGreen*pNotInGreen)
1 - pnorm(0, avg, se)
```

1.[RD] random variables' distribution Monte Carlo vs. CLT second

```{r}
pInGreen = green/(green+black+red)
pNotInGreen <- 1 - pInGreen
winamount <- 17
looseAmount <- -1
n <- 10000
B <- 10000
set.seed(1)
S <- replicate(B,{  
  X <- sample(c(winamount,looseAmount), size = n, replace = TRUE, prob = c(pInGreen, pNotInGreen))
  mean(X)
})
mean(S)
sd(S)
mean(S>0) # p of winning more than $0 as estimated by your Monte Carlo simulation

```


```{r}
incrct <- -0.25 
crct <- 1
questN <- 44
mltplAnswr <- 5
p <- 1/mltplAnswr
p # p of guessing correctly for 1 question
mu <- crct * p + incrct *(1 - p) 
mu # E value of points for guessing on 1 question
ETot <- questN*mu 
ETot # E score of guessing on  44 questions
sigma <- sqrt(questN) * abs(incrct-crct) * sqrt(p*(1-p))
sigma # SE of guessing on all 44 questions
pScore8 <- 1-pnorm(8,mu,sigma) 
set.seed(21,sample.kind = "Rounding")
B <- 10000
examScores <- replicate(B,{ # Monte Carlo simulation
  X <- sample(c(crct, incrct), questN, replace = TRUE, prob=c(p, 1-p))
  sum(X)
})
mean(examScores>=8)# CLT to determine the p that a guessing student scores 8 points or higher on the SAT
```
```{r}
incrct <- 0
crct <- 1
questN <- 44
mltplAnswr <- 4
p <- 1/mltplAnswr
mu <- (crct*p)+(incrct*(1-p))# E value of 1 question
mu*questN # E value of test
p <- seq(0.25, 0.95, 0.05) # a range of correct answer probabilities representing a range of student skills
E <- sapply(p, function(x) {
  mu <- questN* (crct*x + incrct*(1-x))
  sigma <- sqrt(questN) * abs(incrct-crct) * sqrt(x*(1-x))
  1 - pnorm(35, mu, sigma)
})
plot(p, E)
p[which(E > 0.8)]
min(p[which(E > 0.8)]) # lowest p such that the Pr of scoring over 35 exceeds 80%
```

###### ^1^ Note: mu = n * (a*p + b*(1-p))
###### ^1^ Note: sigma = sqrt(n) * abs(b-a) * sqrt(p*(1-p))

```{r}
nLoan <- 1000
loanAmount <- 180000
lossPerForeclosure <- -200000
nPay <- 0
pay <- 1
p <- 0.02
defaults <- sample( c(nPay,pay), nLoan, prob=c(1-p, p), replace = TRUE)
sum(defaults * lossPerForeclosure) #random variable 
```

>load

```{r}
library(tidyverse)
```

```{r}
nLoan <- 1000
loanAmount <- 180000
lossPerForeclosure <- -200000
nPay <- 0
pay <- 1
p <- 0.02
B <- 10000
losses <- replicate(B, { #Monte Carlo simulation
  defaults <- sample( c(nPay,pay), nLoan, prob=c(1-p, p), replace = TRUE) 
  sum(defaults * lossPerForeclosure)
}) 
data.frame(lossesInMillions = losses/10^6) %>%
  ggplot(aes(lossesInMillions)) + # E loses - values 
  geom_histogram(binwidth = 0.6, col = "black")
```

###### ^1^ Note: CLT says need of Monte Carlo ->  losses are a sum of independent draws, its distribution is approximately normal with E and SE

```{r}
nLoan <- 1000
loanAmount <- 180000
lossPerForeclosure <- -200000
nPay <- 0
pay <- 1
p <- 0.02
nLoan*(p*lossPerForeclosure + (1-p)*0)    # E
sqrt(nLoan)*abs(lossPerForeclosure)*sqrt(p*(1-p))    # SE
x = - lossPerForeclosure*p/(1-p) 
x
intrRate <- x/loanAmount #  interest rate 
```

###### ^1^ Note:  set an interest rate to guarantee that on average, one breaks even -> add quantity x to each loan, represented by draws so that the expected values equals zero 
###### ^1^ Note: still there's a 50% chance that one will lose money -> set an interest rate that makes it unlikely for this to happen -> interest rate for 0.01 p of losing money -> x have to be now? -> the sum, S, to have the p of S < than zero to be 0.01

```{r}
nLoan <- 1000
loanAmount <- 180000
lossPerForeclosure <- -200000
nPay <- 0
pay <- 1
p <- 0.02
z <- qnorm(0.01)
x <- -lossPerForeclosure*( nLoan*p - z*sqrt(nLoan*p*(1-p)))/ ( nLoan*(1-p) + z*sqrt(nLoan*p*(1-p)))
intrRate <- x/loanAmount    # interest rate
lossPerForeclosure*p + x*(1-p)    # E value of the profit per loan
nLoan*(lossPerForeclosure*p + x*(1-p)) # E value of the profit over n loans
```

```{r}
nLoan <- 1000
loanAmount <- 180000
lossPerForeclosure <- -200000
nPay <- 0
pay <- 1
p <- 0.02
B <- 100000
profit <- replicate(B, { # Monte Carlo simulation
  draws <- sample( c(x, lossPerForeclosure), nLoan, 
                   prob=c(1-p, p), replace = TRUE) 
  sum(draws)
})
mean(profit)    # E value of the profit over n loans
mean(profit<0)    # p of losing money
```

###### ^1^ Note:  default p rate is twice high, say 4% -> minimize the chances of losing money by increasing loan number-> then one will get a positive E value by making n large = claiming to minimize the SE -> to hold,then one person defaults must be independent of other people defaulting

```{r}
B <- 10000
nLoan <- 1000 
loanAmount <- 180000
lossPerForeclosure <- -200000
nPay <- 0
pay <- 1
p <- 0.04
r <- 0.05
x <- r*loanAmount
lossPerForeclosure*p + x*(1-p)
z <- qnorm(0.01)
n <- ceiling((z^2*(x-lossPerForeclosure)^2*p*(1-p))/(lossPerForeclosure*p + x*(1-p))^2)
n    # number of loans required
n*(lossPerForeclosure*p + x * (1-p))    # E profit over n loans
profit <- replicate(B, { # Monte Carlo simulation with known default p
  draws <- sample( c(x, lossPerForeclosure), n, 
                   prob=c(1-p, p), replace = TRUE) 
  sum(draws)
})
mean(profit)
profit <- replicate(B, { # Monte Carlo simulation with unknown default p
  newP <- p + sample(seq(-0.01, 0.01, length = 100), 1)
  draws <- sample( c(x, lossPerForeclosure), n, 
                   prob=c(1-newP, newP), replace = TRUE)
  sum(draws)
})
mean(profit)    # E profit
mean(profit < 0)    # p of losing money
mean(profit < -10000000)    # p of losing over $10 million
```
```{r}
n <- 10000
lossPerForeclosure <- -200000
p <- 0.03
set.seed(1) # sure your answer matches the expected result after random sampling
defaults <- sample( c(0,1), n, replace = TRUE, prob=c(1-p, p))# the default outcomes of `n` loans
S <- sum(defaults * lossPerForeclosure) # the total amount of money lost across all foreclosures
S
B <- 10000 # the number of times we want the simulation to run
S <- replicate(B, { # Monte Carlo generates a list of summed losses for 'n' loans
  defaults <- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE) 
  sum(defaults * lossPerForeclosure)
})
hist(S) 
```
```{r}
n*(p*lossPerForeclosure + (1-p)*0) # E loss due to default out of 10,000 loans
sqrt(n) * abs(lossPerForeclosure) * sqrt(p*(1 - p))# SE of the sum of 10,000 loans 
x <- -(lossPerForeclosure*p) / (1 - p) # total amount necessary to have an expected outcome of $0
x/loanAmount # Convert `x` to a rate, given that the loan amount is $180,000
z <- qnorm(0.05) 
x <- -lossPerForeclosure*( n*p - z*sqrt(n*p*(1 - p)))/ ( n*(1 - p) + z*sqrt(n*p*(1 - p)))
x / loanAmount # Convert `x` to an interest rate, given that the loan amount is $180,000
```

>load

```{r}
library(tidyverse)
library(dslabs)
data(death_prob)
head(death_prob)
```

```{r}
pF <- death_prob %>% 
  filter(age==50, sex=="Female")%>%
  pull(prob)
plotPF <- ggplot(death_prob, aes(x=age, y=prob)) +
  geom_line(aes(col=sex)) +
  theme(panel.background = element_blank()) +
  xlab("age") +
  ylab("probability") +
  ggtitle("Death probability male vs. female by age") 
plotPF
```

```{r}
age25 <- death_prob %>%
  filter(age <= 25)
age50 <- death_prob %>%
  filter(age > 25 & age <=50)
age75 <- death_prob %>%
  filter(age > 50 & age <=75)
age100 <- death_prob %>%
  filter(age > 75 & age <=100)
```

>load

```{r}
library(gridExtra)
```

```{r}
manual_color <- c("Male" = "#e50914", "Female" = "#34a853")
plotProb1 <- ggplot(age25, aes(x=age, y=prob)) +
  geom_line(aes(col=sex)) + 
  theme(axis.text = element_text(face = "bold"), panel.background = element_blank()) +
  scale_color_manual(values = manual_color) +
  xlab("age") +
  ylab("probability") +
  ggtitle("Death probability - age<=25") 
plotProb2 <- ggplot(age50, aes(x=age, y=prob)) +
  geom_line(aes(col=sex)) +
  theme(axis.text = element_text(face = "bold"), panel.background = element_blank()) +
  scale_color_manual(values = manual_color) +
  xlab("age") +
  ylab("probability") +
  ggtitle("Death probability - age: 26 to 50") 
```

```{r}
grid.arrange(plotProb1, plotProb2, ncol=1)
plotProb3 <- ggplot(age75, aes(x=age, y=prob)) +
  geom_line(aes(col=sex)) +
  scale_color_manual(values = manual_color) +
  theme(axis.text = element_text(face = "bold"), panel.background = element_blank()) +
  xlab("age") +
  ylab("probability") +
  ggtitle("Death probability - age: 51 to 75") 
plotProb4 <- ggplot(age100, aes(x=age, y=prob)) +
  geom_line(aes(col=sex)) +
  scale_color_manual(values = manual_color) +
  theme(axis.text = element_text(face = "bold"), panel.background = element_blank()) +
  # theme_minimal(base_size = 12) +
  xlab("age") +
  ylab("probability") +
  ggtitle("Death probability - age: 76 to 100") 
grid.arrange(plotProb3, plotProb4, ncol=1)
```

```{r}
nPolicies<- 1000
loss <- -150000
gain <- 1150
loss*p+gain*(1-pF)
abs(loss-gain)*sqrt(pF*(1-pF))# SE of the profit on one policy for a 50 year old female
nPolicies*(loss*pF+gain*(1-pF))# E of the company's profit over all 1,000 policies for 50 year old females
sqrt(nPolicies)*(abs(loss-gain)*sqrt(pF*(1-pF)))# SE of the sum of the expected value over all 1,000 policies for 50 year old females
pnorm(0,nPolicies*(loss*pF+gain*(1-pF)), sqrt(nPolicies)*(abs(loss-gain)*sqrt(pF*(1-pF))))# CLT to calculate the Pr that the insurance company loses money on this set of 1,000 policies
```
```{r}
nPolicies<- 1000
loss <- -150000
gain <- 1150
EProfit <- 700000
pLoss <- 0.015
pM <- death_prob%>% #  determine the p of death within one year for a 50 year old male
  filter(age==50,sex=="Male")%>%pull(prob)
pM
b <- ((EProfit/nPolicies)- loss*pM)/(1-pM)
b
serr <- sqrt(nPolicies)*abs(b-loss)*sqrt(pM*(1-pM))# SE  of the sum of 1,000 premiums
serr
pnorm(0,nPolicies*(loss*pM+b*(1-pM)),serr) # p of losing money on a series of 1,000 policies to 50 year old males
mu <- nPolicies * (loss*pLoss+ gain*(1-pLoss))
mu
serr <- sqrt(nPolicies)*abs(loss - gain)*sqrt(pLoss*(1-pLoss))# SE of the E value of the company's profits over 1,000 policies
serr
pnorm(0,mu,serr)# p of the company losing money
pnorm(-1000000,mu, serr)# p of losing more than $1 million
```

```{r}
p <- seq(.01, .03, .001)
loss <- -150000
gain <- 1150
n <- 1000
pLoseMoney <- sapply(p, function(p){
  expVal <- n*(loss*p + gain*(1-p))
  se <- sqrt(n) * abs(gain-loss) * sqrt(p*(1-p))
  pnorm(0, expVal, se)
})
data.frame(p, pLoseMoney) %>%
  filter(pLoseMoney > 0.9) %>%
  pull(p) %>%
  min()
```

```{r}
loss <- -150000
gain <- 1150
n <- 1000
pLoseMillion <- sapply(p, function(p){
  expVal <- n*(loss*p + gain*(1-p))
  se <- sqrt(n) * abs(gain-loss) * sqrt(p*(1-p))
  pnorm(-1*10^6, expVal, se)
})
data.frame(p, pLoseMillion) %>%
  filter(pLoseMillion > 0.9) %>%
  pull(p) %>%
  min()
```

```{r}
set.seed(25)
loss <- -150000
gain <- 1150
n <- 1000
pLoss <- 0.015
X <- sample(c(0,1),n, replace = TRUE, prob=c((1-pLoss),pLoss)) 
loss <- loss*sum(X==1)/10^6
profit <- gain*sum(X==0)/10^6
loss+profit
```

```{r}
B <- 10000
loss <- -150000
gain <- 1150
pLoss <- 0.015
n <- 1000
set.seed(27)
S <- replicate(B,
               {
                 X <- sample(c(0,1),n, replace = TRUE, prob=c((1-pLoss),pLoss)) 
                 loss<- loss*sum(X==1)/10^6
                 profit<- gain*sum(X==0)/10^6
                 loss+profit
                 
               })
sum(S<=-1)/10000
```


```{r}
pLoss <- 0.015
n <- 1000
loss <- -150000
z <- qnorm(.05)
x <- loss*(n*pLoss-z*sqrt(n*pLoss*(1-pLoss)))/(n*(1-pLoss)+z*sqrt(n*pLoss*(1-pLoss)))
x
loss*pLoss + x*(1-pLoss) # E profit per policy at this rate
n*(loss*pLoss + x*(1-pLoss)) # E profit over 1,000 policies
```

```{r}
B <- 10000
pLoss <- 0.015
set.seed(28)
S <- replicate(B, {
  X <- sample(c(0,1), n, replace = TRUE, prob=c((1-pLoss), pLoss))
  loss <- l*sum(X==1)/10^6 
  profit <- x*sum(X==0)/10^6
  loss+profit
})
sum(S<0)/B
```

```{r}
pLoss <- 0.015
n <- 1000
loss <- -150000
B <- 10000
x <- 3268
set.seed(29,sample.kind="Rounding")
X <- replicate(B,{
  nextP <- pLoss+sample(seq(-0.01, 0.01, length=100),1)
  Y <- sample(c(x,l),n,replace=TRUE,prob=c(1-nextP,nextP))
  sum(Y)
})
mean(X) # E value over 1,000 policies
sum(X<0)/B # p of losing money
mean(X < -1000000)# p of losing more than $1 million
```

# VI. Inference

> load 

```{r}
library(tidyverse)
library(dslabs)
```

1.[sample][estimate][SE][Xbar close to p]

```{r}
take_poll(25) 
X_hat <- 0.48
se_estimate <- sqrt(X_hat*(1-X_hat)/25)
pnorm(0.01/se_estimate) - pnorm(-0.01/se_estimate) 
```
1.[sample][estimate][SE][Xbar close to p]

```{r}
take_poll(2000) 
X_hat <- 0.48
se_estimate <- sqrt(X_hat*(1-X_hat)/2000)
pnorm(0.01/se_estimate) - pnorm(-0.01/se_estimate) 
```

1.[SE]versus [p]

```{r}
N <- 25
p <- seq(0,1, length =100)
se <- sqrt(p*(1-p)/N)
plot(p,se)
```

1.[SE] versus [p] multiple

```{r}
p <- seq(0, 1, length = 100)
sample_sizes <- c(25, 100, 1000)
for(N in sample_sizes){
  se <- sqrt(p*(1-p)/N)
  plot(p,se,ylim = c(0,0.5/sqrt(25)))
}
```
1.[SE]of spread 

```{r}
N <- 25
p <- 0.45
2*sqrt(p*(1-p)/N)
```


2. [Monte Carlo]
```{r}
p <- 0.45    
N <- 1000
x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
x_hat <- mean(x)
B <- 10000    
N <- 1000 
x_hat <- replicate(B, {
  x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  mean(x) # Monte Carlo 
})
mean(x_hat)
sd(x_hat)
```

2.[histogram]

>load 

```{r}
library(tidyverse)
library(gridExtra)
```

```{r}
p1 <- data.frame(x_hat = x_hat) %>%
  ggplot(aes(x_hat)) +
  geom_histogram(binwidth = 0.005, color = "black")
p2 <- data.frame(x_hat = x_hat) %>%
  ggplot(aes(sample = x_hat)) +
  stat_qq(dparams = list(mean = mean(x_hat), sd = sd(x_hat))) +
  geom_abline() +
  ylab("X_hat") +
  xlab("Theoretical normal")
grid.arrange(p1, p2, nrow=1)
```

3. [sample average]

```{r}
take_sample <- function(p,N){
  X <- sample(c(0,1), size = N,replace = TRUE, prob =c(1-p,p))
  mean(X)
}
set.seed(1)
p <- 0.45
N <- 100
take_sample(p,N)
```

4. [distribution errors]

```{r}
p <- 0.45
N <- 100
B <- 10000
set.seed(1)
errors <- replicate(B, p- take_sample(p,N))
mean(errors)
```

5.[average size of error]

```{r}
p <- 0.45
N <- 100
B <- 10000
set.seed(1)
errors <- replicate(B, p - take_sample(p, N))
mean(abs(errors))
```

6.[standard deviation of spread]

```{r}
p <- 0.45
N <- 100
B <- 10000
set.seed(1)
errors <- replicate(B, p - take_sample(p, N))
sqrt(mean(errors^2))
```

7. [estimating standard error]

```{r}
p <- 0.45
N <- 100
sqrt(p*(1-p)/N)
```

8.[standard estimate of the error]

```{r}
p <- 0.45
N <- 100
set.seed(1)
X <- sample(0:1,N, replace =T,p=c(1-p,p))
X_bar <- mean(X)
sqrt(X_bar*(1-X_bar)/N)
```

9.[plot errors]

```{r}
p <- 0.45
N <- 100
B <- 10000
set.seed(1)
errors <- replicate(B, p - take_sample(p, N))
qqnorm(errors)
qqline(errors)
```

10.[estimating probability of x bar value]

```{r}
p <- 0.45
N <- 100
1-pnorm(0.5,mean=p,sd  = sqrt(p*(1-p)/N))
```

11.[estimating probability of error size]
 
```{r}
N <-100
X_hat <- 0.51
se_hat <- sqrt(X_hat*(1-X_hat)/N)
1 - pnorm (0.01,0, se_hat) + pnorm(-0.01,0,se_hat)
```
 
 
12.[Monte Carlo] confidence intervals 

```{r}
p <- 0.45
N <- 1000
X <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))    # generate N observations
X_hat <- mean(X)    # calculate X_hat
SE_hat <- sqrt(X_hat*(1-X_hat)/N)    # calculate SE_hat, SE of the mean of N observations
c(X_hat - 2*SE_hat, X_hat + 2*SE_hat)    # build interval of 2*SE above and below mean
```
 
13.[confidence interval] solve for z 

```{r}
z <- qnorm(0.995)    # calculate z to solve for 99% confidence interval
pnorm(qnorm(0.995))    # demonstrating that qnorm gives the z value for a given probability
pnorm(qnorm(1-0.995))    # demonstrating symmetry of 1-qnorm
pnorm(z) - pnorm(-z) 
```

 
14. [Monte Carlo] confirm that a 95% confidence intervals includes p 95%

```{r}
B <- 10000
inside <- replicate(B, {
  X <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  X_hat <- mean(X)
  SE_hat <- sqrt(X_hat*(1-X_hat)/N)
  between(p, X_hat - 2*SE_hat, X_hat + 2*SE_hat)    # TRUE if p in confidence interval
})
mean(inside)

```

15. [confidence interval] of spread 

```{r}
N <- 25
X_hat <- 0.48
(2*X_hat - 1) + c(-2, 2)*2*sqrt(X_hat*(1-X_hat)/N)
```

16.[p-value] for spread of 0.02

```{r}
N <- 100    # sample size
z <- sqrt(N) * 0.02/0.5    # spread of 0.02
1 - (pnorm(z) - pnorm(-z))
```


17.[confidence interval] for p

```{r}
data(polls_us_election_2016)
polls <- filter(polls_us_election_2016, enddate >= "2016-10-31" & state == "U.S.")
nrow(polls)
view(polls)
N <- head(polls$samplesize,1)
N
X_hat <- (head(polls$rawpoll_clinton,1)/100)
X_hat
se_hat <- sqrt(X_hat*(1-X_hat)/N)
se_hat
qnorm(0.975)
ci <- c(X_hat - qnorm(0.975)*se_hat, X_hat + qnorm(0.975)*se_hat)
```

18. polster result for p

```{r}
data(polls_us_election_2016)
polls <- filter(polls_us_election_2016, enddate >= "2016-10-31" & state == "U.S.")
nrow(polls)
view(polls)
N <- head(polls$samplesize,1)
N
X_hat <- (head(polls$rawpoll_clinton,1)/100)
X_hat
se_hat <- sqrt(X_hat*(1-X_hat)/N)
se_hat
qnorm(0.975)
ci <- c(X_hat - qnorm(0.975)*se_hat, X_hat + qnorm(0.975)*se_hat)
# pollster results for p 
head(polls)
polls <- mutate(polls, X_hat = polls$rawpoll_clinton/100, se_hat = sqrt(X_hat*(1-X_hat)/polls$samplesize), lower = X_hat - qnorm(0.975)*se_hat, upper = X_hat + qnorm(0.975)*se_hat)
pollster_results <- select(polls, pollster, enddate, X_hat, se_hat, lower, upper)
```


19. [compare] to actual data 

```{r}
head(pollster_results)
avg_hit <- pollster_results %>% 
  mutate(hit=(lower<0.482 & upper>0.482)) %>% 
  summarize(mean(hit))
avg_hit 
```

20.[confidence interval] d

```{r}
polls <- polls_us_election_2016 %>% filter(enddate >= "2016-10-31" & state == "U.S.")  %>%
  mutate(d_hat = rawpoll_clinton/100 - rawpoll_trump/100)
N <- polls$samplesize[1]
N
d_hat <- polls$d_hat[1]
d_hat
X_hat <- (d_hat + 1) /2
se_hat <- 2*sqrt(X_hat*(1-X_hat)/N)
se_hat
ci <- c(d_hat - qnorm(0.975)*se_hat, d_hat + qnorm(0.975)*se_hat)
```

21. [d] pollster data

```{r}
head(polls)
pollster_results <- polls %>% mutate(X_hat = (d_hat + 1) / 2) %>% mutate(se_hat = 2 * sqrt(X_hat * (1 - X_hat) / samplesize)) %>% mutate(lower = d_hat - qnorm(0.975) * se_hat) %>% mutate(upper = d_hat + qnorm(0.975) * se_hat) %>% select(pollster, enddate, d_hat, lower, upper)
pollster_results
```


22. [compare]  actual results

```{r}
head(pollster_results)
avg_hit <- pollster_results %>% mutate(hit=lower <= 0.021 & upper >= 0.021) %>% summarize(mean(hit))
```



22. [compare]  actual results by pollsters

```{r}
head(polls)
polls %>% mutate(error = d_hat - 0.021) %>% ggplot(aes(x = pollster, y = error)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1))

```
22. [compare]  actual results by multiple pollsters

```{r}
head(polls)
polls %>% mutate(error = d_hat - 0.021) %>%
  group_by(pollster) %>%
  filter(n() >= 5) %>%
  ggplot(aes(pollster, error)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

23. [simulate] poll

```{r}
head(polls)
polls %>% mutate(error = d_hat - 0.021) %>%
  group_by(pollster) %>%
  filter(n() >= 5) %>%
  ggplot(aes(pollster, error)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

24.[generate spread] combined poll

```{r}
head(polls)
polls %>% mutate(error = d_hat - 0.021) %>%
  group_by(pollster) %>%
  filter(n() >= 5) %>%
  ggplot(aes(pollster, error)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

25.[simulating poll data] generate

```{r}
head(polls)
polls %>% mutate(error = d_hat - 0.021) %>%
  group_by(pollster) %>%
  filter(n() >= 5) %>%
  ggplot(aes(pollster, error)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


26.[pollster bias] investigate

```{r}
head(polls)
polls %>% mutate(error = d_hat - 0.021) %>%
  group_by(pollster) %>%
  filter(n() >= 5) %>%
  ggplot(aes(pollster, error)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

27. [data driven model]

```{r}
one_poll_per_pollster <- polls %>% group_by(pollster) %>%
  filter(enddate == max(enddate)) %>%      # keep latest poll
  ungroup()
one_poll_per_pollster %>%
  ggplot(aes(spread)) + geom_histogram(binwidth = 0.01)
results <- one_poll_per_pollster %>%
  summarize(avg = mean(spread), se = sd(spread)/sqrt(length(spread))) %>%
  mutate(start = avg - 1.96*se, end = avg + 1.96*se)
round(results*100, 1)
```
>load

```{r}
library(dslabs)
```


28. [heights] revisited 

```{r}
data(heights)
x <- heights %>% filter(sex == "Male") %>%
  .$height
mean(x)
sd(x)
head(x)
set.seed(1)
N <- 50
X <- sample(x,N,replace =TRUE)
mean(X)
sd(X)
```



29.[confidence interval] calculate 

```{r}
head(x)
set.seed(1)
N <- 50
X <- sample(x, N, replace = TRUE)
X_hat <- mean(X)
se_hat<- sd(X)
se <- se_hat/sqrt(N)
se
ci <- c(qnorm(0.025,mean(X),se),qnorm(0.975,mean(X),se))
```

30. [Monte Carlo]

```{r}
mu <- mean(x)
set.seed(1)
N <- 50
B <- 10000
res <- replicate(B, {
  X <- sample(x, N, replace = TRUE)
  X_hat <- mean(X)
  se_hat <- sd(X)
  se <- se_hat / sqrt(N)
  interval <- c(qnorm(0.025, mean(X), se) , qnorm(0.975, mean(X), se))
  between(mu, interval[1], interval[2])
})
mean(res)
```


31. [polling bias]

>load

```{r}
library(dslabs)
library(dplyr)
library(ggplot2)
```


```{r}
library(dslabs)
library(dplyr)
library(ggplot2)
data("polls_us_election_2016")
polls <- polls_us_election_2016 %>% 
  filter(pollster %in% c("Rasmussen Reports/Pulse Opinion Research","The Times-Picayune/Lucid") &
           enddate >= "2016-10-15" &
           state == "U.S.") %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) 
polls%>% ggplot(aes(pollster,spread))+geom_boxplot()+geom_point()
```
 
32. [estimate]compute

```{r}
head(polls)
polls %>% group_by(pollster)
sigma <- polls %>% group_by(pollster) %>% summarize(s = sd(spread))
sigma
```


33. [confidence interval]calculate

```{r}
head(polls)
res <- polls %>% group_by(pollster) %>% summarize(avg=mean(spread), s = sd(spread), N=n())
res
estimate <- max(res$avg) - min(res$avg)
estimate
se_hat <- sqrt(res$s[2]^2/res$N[2] + res$s[1]^2/res$N[1])
se_hat
ci <- c(estimate - qnorm(0.975)*se_hat, estimate + qnorm(0.975)*se_hat)
```

34. [p-value]calculate

```{r}
res <- polls %>% group_by(pollster) %>% 
  summarize(avg = mean(spread), s = sd(spread), N = n()) 
estimate <- res$avg[2] - res$avg[1]
se_hat <- sqrt(res$s[2]^2/res$N[2] + res$s[1]^2/res$N[1])
2* (1-pnorm(estimate/se_hat,0,1))
```

35. [variability]compare

```{r}
polls <- polls_us_election_2016 %>% 
  filter(enddate >= "2016-10-15" &
           state == "U.S.") %>%
  group_by(pollster) %>%
  filter(n() >= 5) %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
  ungroup()
var <- polls %>% group_by(pollster) %>% summarize(avg = mean(spread), s = sd(spread))
var
```
36. [spread, average of spread, estimate of standard deviation]

>load

```{r}
library(dplyr)
library(dslabs)
```


```{r}
data(polls_us_election_2016)
polls <- polls_us_election_2016 %>% 
  filter(state == "Florida" & enddate >= "2016-11-04" ) %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)
head(polls)
results <- polls %>% summarize(avg = mean(spread),  se = sd(spread)/sqrt(n()))
results
```


37. [posterior distribution] calculation 


```{r}
results
mu <- 0
tau <- 0.01
sigma <- results$se
Y <- results$avg
tau <- 0.01
miu <- 0
B <- sigma^2 / (sigma^2 + tau^2)
B
miu + (1 - B) * (Y - miu)
```
38. [posterior distribution] SE 



```{r}
mu <- 0
tau <- 0.01
sigma <- results$se
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau^2)
sqrt(1 / (1 / sigma ^2 + 1 / tau ^2))
```

39. [confidence interval] credible 


```{r}
mu <- 0
tau <- 0.01
sigma <- results$se
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau^2)
se <- sqrt( 1/ (1/sigma^2 + 1/tau^2))
est <- B * mu + (1 - B) * Y
est
ci <- c(est - qnorm(0.975) * se, est + qnorm(0.975) * se)
ci
```


39. [probability] spread<0 

```{r}
exp_value <- B*mu + (1-B)*Y 
se <- sqrt( 1/ (1/sigma^2 + 1/tau^2))
pnorm(0, exp_value, se)
```

40. [variance] change

```{r}
mu <- 0
sigma <- results$se
Y <- results$avg
taus <- seq(0.005, 0.05, len = 100)
p_calc <- function(tau) {
  B <- sigma ^ 2 / (sigma^2 + tau^2)
  se <- sqrt(1 / (1/sigma^2 + 1/tau^2))
  exp_value <- B * mu + (1 - B) * Y
  pnorm(0, exp_value, se)
}
ps <- p_calc(taus)
plot(taus, ps)
```


41. [variance] biased

```{r}
mu <- 0
tau <- 0.035
sigma <- results$se
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau^2)
posterior_mean <- B*mu + (1-B)*Y
posterior_se <- sqrt(1 / (1/sigma^2 + 1/tau^2))
posterior_mean
posterior_se
posterior_mean + c(-1.96, 1.96)*posterior_se
1 - pnorm(0, posterior_mean, posterior_se)
```


42. [probability] calculating of d>0 with general bias

```{r}
mu <- 0
tau <- 0.035
sigma <- sqrt(results$se^2 + .025^2)
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau^2)
posterior_mean <- B*mu + (1-B)*Y
posterior_se <- sqrt(1 / (1/sigma^2 + 1/tau^2))
1 - pnorm(0, posterior_mean, posterior_se)
```

43. [prediction] 

>load

```{r}
library(tidyverse)
library(dslabs)
```

```{r}
data("polls_us_election_2016")
head(results_us_election_2016)
results_us_election_2016 %>% arrange(desc(electoral_votes)) %>% top_n(5, electoral_votes)
results <- polls_us_election_2016 %>%
  filter(state != "U.S." &
           !grepl("CD", state) &
           enddate >= "2016-10-31" &
           (grade %in% c("A+", "A", "A-", "B+") | is.na(grade))) %>%
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
  group_by(state) %>%
  summarize(avg = mean(spread), sd = sd(spread), n = n()) %>%
  mutate(state = as.character(state))
results %>% arrange(abs(avg))
results <- left_join(results, results_us_election_2016, by="state")
results_us_election_2016 %>% filter(!state %in% results$state)
results <- results %>%
  mutate(sd = ifelse(is.na(sd), median(results$sd, na.rm = TRUE), sd))
mu <- 0
tau <- 0.02
results %>% mutate(sigma = sd/sqrt(n),
                   B = sigma^2/ (sigma^2 + tau^2),
                   posterior_mean = B*mu + (1-B)*avg,
                   posterior_se = sqrt( 1 / (1/sigma^2 + 1/tau^2))) %>%
  arrange(abs(posterior_mean))
mu <- 0
tau <- 0.02
clinton_EV <- replicate(1000, {
  results %>% mutate(sigma = sd/sqrt(n),
                     B = sigma^2/ (sigma^2 + tau^2),
                     posterior_mean = B*mu + (1-B)*avg,
                     posterior_se = sqrt( 1 / (1/sigma^2 + 1/tau^2)),
                     simulated_result = rnorm(length(posterior_mean), posterior_mean, posterior_se),
                     clinton = ifelse(simulated_result > 0, electoral_votes, 0)) %>%    
    summarize(clinton = sum(clinton)) %>%    
    .$clinton + 7    
})
mean(clinton_EV > 269) 
mu <- 0
tau <- 0.02
bias_sd <- 0.03
clinton_EV_2 <- replicate(1000, {
  results %>% mutate(sigma = sqrt(sd^2/(n) + bias_sd^2),    
                     B = sigma^2/ (sigma^2 + tau^2),
                     posterior_mean = B*mu + (1-B)*avg,
                     posterior_se = sqrt( 1 / (1/sigma^2 + 1/tau^2)),
                     simulated_result = rnorm(length(posterior_mean), posterior_mean, posterior_se),
                     clinton = ifelse(simulated_result > 0, electoral_votes, 0)) %>%    
    summarize(clinton = sum(clinton)) %>%    
    .$clinton + 7    
})
mean(clinton_EV_2 > 269)   

```



44. [variability] 

```{r}
one_pollster <- polls_us_election_2016 %>%
  filter(pollster == "Ipsos" & state == "U.S.") %>%
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)
se <- one_pollster %>%
  summarize(empirical = sd(spread),
            theoretical = 2*sqrt(mean(spread)*(1-mean(spread))/min(samplesize)))
se
one_pollster %>% ggplot(aes(spread)) +
  geom_histogram(binwidth = 0.01, color = "black")
```


45. [trend] 

```{r}
polls_us_election_2016 %>%
  filter(state == "U.S." & enddate >= "2016-07-01") %>%
  group_by(pollster) %>%
  filter(n() >= 10) %>%
  ungroup() %>%
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
  ggplot(aes(enddate, spread)) +
  geom_smooth(method = "loess", span = 0.1) +
  geom_point(aes(color = pollster), show.legend = FALSE, alpha = 0.6)
```


46. [% across time] 

```{r}
polls_us_election_2016 %>%
  filter(state == "U.S." & enddate >= "2016-07-01") %>%
  select(enddate, pollster, rawpoll_clinton, rawpoll_trump) %>%
  rename(Clinton = rawpoll_clinton, Trump = rawpoll_trump) %>%
  gather(candidate, percentage, -enddate, -pollster) %>%
  mutate(candidate = factor(candidate, levels = c("Trump", "Clinton"))) %>%
  group_by(pollster) %>%
  filter(n() >= 10) %>%
  ungroup() %>%
  ggplot(aes(enddate, percentage, color = candidate)) +
  geom_point(show.legend = FALSE, alpha = 0.4) +
  geom_smooth(method = "loess", span = 0.15) +
  scale_y_continuous(limits = c(30, 50))
```
47. [confidence intervals] 

>load

```{r}
library(dplyr)
library(dslabs)
```


```{r}
library(dplyr)
library(dslabs)
data("polls_us_election_2016")
polls <- polls_us_election_2016 %>% 
  filter(state != "U.S." & enddate >= "2016-10-31") %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)
cis <- polls %>% mutate(X_hat = (spread+1)/2, se = 2*sqrt(X_hat*(1-X_hat)/samplesize), 
                        lower = spread - qnorm(0.975)*se, upper = spread + qnorm(0.975)*se) %>%
  select(state, startdate, enddate, pollster, grade, spread, lower, upper)
```


48. [compare] 


```{r}
add <- results_us_election_2016 %>% mutate(actual_spread = clinton/100 - trump/100) %>% select(state, actual_spread)
ci_data <- cis %>% mutate(state = as.character(state)) %>% left_join(add, by = "state")
p_hits <- ci_data %>% mutate(hit = lower <= actual_spread & upper >= actual_spread) %>% summarize(proportion_hits = mean(hit))
p_hits
```
49. [stratify] 


```{r}
add <- results_us_election_2016 %>% mutate(actual_spread = clinton/100 - trump/100) %>% select(state, actual_spread)
ci_data <- cis %>% mutate(state = as.character(state)) %>% left_join(add, by = "state")
p_hits <- ci_data %>% mutate(hit = lower <= actual_spread & upper >= actual_spread) %>% 
  group_by(pollster) %>%
  filter(n() >=  5) %>%
  summarize(proportion_hits = mean(hit), n = n(), grade = grade[1]) %>%
  arrange(desc(proportion_hits))
p_hits
```

50. [stratify] 

```{r}
add <- results_us_election_2016 %>% mutate(actual_spread = clinton/100 - trump/100) %>% select(state, actual_spread)
ci_data <- cis %>% mutate(state = as.character(state)) %>% left_join(add, by = "state")
p_hits <- ci_data %>% mutate(hit = lower <= actual_spread & upper >= actual_spread) %>% 
  group_by(state) %>%
  filter(n() >=  5) %>%
  summarize(proportion_hits = mean(hit), n = n()) %>%
  arrange(desc(proportion_hits)) 
p_hits
```

51. [prediction] plot

```{r}
head(p_hits)
p_hits %>% mutate(state = reorder(state, proportion_hits)) %>%
  ggplot(aes(state, proportion_hits)) + 
  geom_bar(stat = "identity") +
  coord_flip()
```


52. [prediction] 

```{r}
head(cis)
errors <- cis %>% dplyr::mutate(error = spread - actual_spread, hit = sign(spread) == sign(actual_spread))
tail(errors)
```

52. [prediction] plot

```{r}
errors <- cis %>% mutate(error = spread - actual_spread, hit = sign(spread) == sign(actual_spread))
p_hits <- errors %>%  group_by(state) %>%
  filter(n() >=  5) %>%
  summarize(proportion_hits = mean(hit), n = n())
p_hits %>% mutate(state = reorder(state, proportion_hits)) %>%
  ggplot(aes(state, proportion_hits)) + 
  geom_bar(stat = "identity") +
  coord_flip()
```

53. [plot] errors

```{r}
head(errors)
hist(errors$error)
median(errors$error)
```

54. [plot] bias

```{r}
head(errors)
errors %>% filter(grade %in% c("A+","A","A-","B+") | is.na(grade)) %>%
  mutate(state = reorder(state, error)) %>%
  ggplot(aes(state, error)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  geom_boxplot() + 
  geom_point()
```

55. [filter] error plot

```{r}

errors %>% filter(grade %in% c("A+","A","A-","B+") | is.na(grade)) %>%
  group_by(state) %>%
  filter(n() >= 5) %>%
  ungroup() %>%
  mutate(state = reorder(state, error)) %>%
  ggplot(aes(state, error)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  geom_boxplot() + 
  geom_point()
```

56. [confidence interval] t - distribution 

```{r}
z <- qt(0.975, nrow(one_poll_per_pollster) - 1)
one_poll_per_pollster %>%
  summarize(avg = mean(spread), moe = z*sd(spread)/sqrt(length(spread))) %>%
  mutate(start = avg - moe, end = avg + moe)
qt(0.975, 14)    
qnorm(0.975)
```

57. [t-distribution] 

```{r}
1 - pt(2, 3) + pt(-2, 3)
```

58. [t-distribution] plot

```{r}
df <- seq(3,50)
pt_func <- function(n) {
  1 - pt(2, n) + pt(-2, n)
}
probs <- sapply(df, pt_func)
plot(df, probs)
```


57. [sample] from normal distribution  

>load

```{r}
library(dslabs)
library(dplyr)
```



```{r}
data(heights)
x <- heights %>% filter(sex == "Male") %>%
  .$height
mu <- mean(x)
N <- 15
B <- 10000
set.seed(1)
res <- replicate(B, {
  X <- sample(x, N, replace=TRUE)
  interval <- mean(X) + c(-1,1)*qnorm(0.975)*sd(X)/sqrt(N)
  between(mu, interval[1], interval[2])
})
mean(res)
```


57. [t-distribution]  sampling from 

```{r}
mu <- mean(x)
set.seed(1)
N <- 15
B <- 10000
res <- replicate(B, {
  s <- sample(x, N, replace = TRUE)
  interval <- c(mean(s) - qt(0.975, N - 1) * sd(s) / sqrt(N), mean(s) + qt(0.975, N - 1) * sd(s) / sqrt(N))
  between(mu, interval[1], interval[2])
})
mean(res)
```



# VI. Regression

> load 

```{r}
library(Lahman)
library(tidyverse)
library(dslabs)
```

```{r}
ds_theme_set()

Teams %>% filter(yearID %in% 1961:2001) %>%
    mutate(HR_per_game = HR / G, R_per_game = R / G) %>%
    ggplot(aes(HR_per_game, R_per_game)) + 
    geom_point(alpha = 0.5)
```
```{r}
Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(SB_per_game = SB / G, R_per_game = R / G) %>%
  ggplot(aes(SB_per_game, R_per_game)) + 
  geom_point(alpha = 0.5)
```

```{r}
Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(BB_per_game = BB / G, R_per_game = R / G) %>%
  ggplot(aes(BB_per_game, R_per_game)) + 
  geom_point(alpha = 0.5)
```

```{r}
Teams %>% filter(yearID %in% 1961:2001 ) %>%
  mutate(AB_per_game = AB/G, R_per_game = R/G) %>%
  ggplot(aes(AB_per_game, R_per_game)) + 
  geom_point(alpha = 0.5)
```


```{r}
Teams %>% filter(yearID %in% 1961:2001 ) %>%
  mutate(E_per_game = E/G, W_per_game = W/G) %>%
  ggplot(aes(E_per_game, W_per_game)) + 
  geom_point(alpha = 0.5)
```


```{r}
Teams %>% filter(yearID %in% 1961:2001 ) %>%
  mutate(X2B_per_game = E/G, X3B_per_game = X3B/G) %>%
  ggplot(aes(X2B_per_game, X3B_per_game)) + 
  geom_point(alpha = 0.5)
```
>load

```{r}
library(tidyverse)
library(HistData)
```


```{r}
data("GaltonFamilies")
set.seed(1983)
galton_heights <- GaltonFamilies %>%
  filter(gender == "male") %>%
  group_by(family) %>%
  sample_n(1) %>%
  ungroup() %>%
  select(father, childHeight) %>%
  rename(son = childHeight)
galton_heights %>%
  summarize(mean(father), sd(father), mean(son), sd(son))
galton_heights %>%
  ggplot(aes(father, son)) +
  geom_point(alpha = 0.5)
```

1.[correlation coefficient]

>load 

```{r}
library(HistData)
```

```{r}
data("GaltonFamilies")
galton_heights <- GaltonFamilies %>%
  filter(gender == "male" & childNum == 1) %>%
  select(father, childHeight) %>%
  rename(son = childHeight)
galton_heights %>% summarize(r = cor(father, son)) %>% 
  pull(r)
```


2.[sample correlation]

```{r}
R <- sample_n(galton_heights, 25, replace = TRUE) %>% # R = random variable 
  summarize(r = cor(father, son))
R
B <- 1000
N <- 25
R <- replicate(B, {
  sample_n(galton_heights, N, replace = TRUE) %>%
    summarize(r = cor(father, son)) %>%
    pull(r)
})
qplot(R, geom = "histogram", binwidth = 0.05, color = I("black"))
mean(R)
sd(R) 
data.frame(R) %>%
  ggplot(aes(sample = R)) +
  stat_qq() +
  geom_abline(intercept = mean(R), slope = sqrt((1-mean(R)^2)/(N-2))) 


```



1.[correlation coefficient]

>load 

```{r}
library(Lahman)
library(tidyverse)
library(dslabs)
```

```{r}
ds_theme_set()
teams_cc <- Teams %>% 
  filter(yearID %in% 1961:2001 )
cor(teams_cc$R/teams_cc$G, teams_cc$AB/teams_cc$G)
```

1.[correlation coefficient]

>load 

```{r}
library(HistData)
```

3.[conditional average]

```{r}
data("GaltonFamilies")
conditional_avg <- galton_heights %>%
  filter(round(father) == 72) %>%
  summarize(avg = mean(son)) %>%
  pull(avg)
conditional_avg
```

4.[stratification]

```{r}
galton_heights %>% mutate(father_strata = factor(round(father))) %>%
  ggplot(aes(father_strata, son)) +
  geom_boxplot() +
  geom_point()
```
5.[boxplot] center

```{r}
galton_heights %>%
  mutate(father = round(father)) %>%
  group_by(father) %>%
  summarize(son_conditional_avg = mean(son)) %>%
  ggplot(aes(father, son_conditional_avg)) +
  geom_point()
```

6.[regression line] calculate

```{r}
mu_x <- mean(galton_heights$father)
mu_y <- mean(galton_heights$son)
s_x <- sd(galton_heights$father)
s_y <- sd(galton_heights$son)
r <- cor(galton_heights$father, galton_heights$son)
m <- r * s_y/s_x
b <- mu_y - m*mu_x
```


6.1.[inverted regression line]

```{r}
mu_x <- mean(galton_heights$father)
mu_y <- mean(galton_heights$son)
s_x <- sd(galton_heights$father)
s_y <- sd(galton_heights$son)
r <- cor(galton_heights$father, galton_heights$son)
m_1 <-  r * s_y / s_x
b_1 <- mu_y - m_1*mu_x
m_2 <-  r * s_x / s_y
b_2 <- mu_x - m_2*mu_y
```


7.[regression line] add

```{r}
galton_heights %>%
  ggplot(aes(father, son)) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = b, slope = m)
```

7.[regression line]z - score

```{r}
galton_heights %>%
  ggplot(aes(scale(father), scale(son))) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = 0, slope = r)
```

8.[correlation] justify

```{r}
galton_heights %>%
  mutate(z_father = round((father - mean(father)) / sd(father))) %>%
  filter(z_father %in% -2:2) %>%
  ggplot() +  
  stat_qq(aes(sample = son)) +
  facet_wrap( ~ z_father)

```

1.[correlation coeeficient, mean, sd]

>load 

```{r}
library(HistData)

```

```{r}
set.seed(1989) #if you are using R 3.5 or earlier
set.seed(1989, sample.kind="Rounding") #if you are using R 3.6 or later
data("GaltonFamilies")
female_heights <- GaltonFamilies%>%     
  filter(gender == "female") %>%     
  group_by(family) %>%     
  sample_n(1) %>%     
  ungroup() %>%     
  select(mother, childHeight) %>%     
  rename(daughter = childHeight)

```

```{r}
female_heights %>%
  summarize(mean(mother), sd(mother), mean(daughter), sd(daughter))
female_heights <- GaltonFamilies %>%
  filter(gender == "female" & childNum == 1) %>%
  select(mother, childHeight) %>%
  rename(daughter = childHeight)
female_heights %>% summarize(r = cor(mother, daughter)) %>% 
  pull(r)
```
[slope, intercept]

```{r}
mu_x <- mean(female_heights$mother)
mu_y <- mean(female_heights$daughter)
s_x <- sd(female_heights$mother)
s_y <- sd(female_heights$daughter)
r <- cor(female_heights$mother, female_heights$daughter)
m <- r * s_y/s_x # Slope = (correlation coefficient of son and father heights) * (standard deviation of sonsâ€™ heights / standard deviation of fathersâ€™ heights)
b <- mu_y - m*mu_x # intercept 
r * s_y/s_x 
```


[percent of the variability ]

```{r}
var_percent<- (r * r)*100
```

[conditional expected value ]

```{r}
x= 60 
m*x+b 
```



## References

